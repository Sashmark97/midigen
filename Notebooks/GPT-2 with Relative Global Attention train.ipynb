{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.nn.modules.normalization import LayerNorm\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.linear import Linear\n",
    "from torch.nn.modules.dropout import Dropout\n",
    "from torch.nn.modules.normalization import LayerNorm\n",
    "from torch.nn.init import *\n",
    "\n",
    "from torch.nn.functional import linear, softmax, dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_START = 0\n",
    "RANGE_NOTE_ON = 128\n",
    "RANGE_NOTE_OFF = 128\n",
    "RANGE_VEL = 32\n",
    "RANGE_TIME_SHIFT = 100\n",
    "\n",
    "START_IDX = {\n",
    "    'note_on': 0,\n",
    "    'note_off': RANGE_NOTE_ON,\n",
    "    'time_shift': RANGE_NOTE_ON + RANGE_NOTE_OFF,\n",
    "    'velocity': RANGE_NOTE_ON + RANGE_NOTE_OFF + RANGE_TIME_SHIFT\n",
    "}\n",
    "\n",
    "\n",
    "SEPERATOR               = \"=========================\"\n",
    "\n",
    "# Taken from the paper\n",
    "ADAM_BETA_1             = 0.9\n",
    "ADAM_BETA_2             = 0.98\n",
    "ADAM_EPSILON            = 10e-9\n",
    "\n",
    "LR_DEFAULT_START        = 1.0\n",
    "SCHEDULER_WARMUP_STEPS  = 4000\n",
    "# LABEL_SMOOTHING_E       = 0.1\n",
    "\n",
    "# DROPOUT_P               = 0.1\n",
    "\n",
    "TOKEN_END               = RANGE_NOTE_ON + RANGE_NOTE_OFF + RANGE_VEL + RANGE_TIME_SHIFT\n",
    "TOKEN_PAD               = TOKEN_END + 1\n",
    "\n",
    "VOCAB_SIZE              = TOKEN_PAD + 1\n",
    "\n",
    "TORCH_FLOAT             = torch.float32\n",
    "TORCH_INT               = torch.int32\n",
    "\n",
    "TORCH_LABEL_TYPE        = torch.long\n",
    "\n",
    "PREPEND_ZEROS_WIDTH     = 4\n",
    "\n",
    "TORCH_CPU_DEVICE = torch.device(\"cpu\")\n",
    "USE_CUDA = 1\n",
    "TORCH_CUDA_DEVICE = torch.device(\"cuda:5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1020/1020 [12:37<00:00,  1.35it/s]\n",
      "100%|██████████| 131/131 [01:33<00:00,  1.40it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_pickle = '/home/storage/3020/db/K_cluster2_backup/TD/data/data_split.pkl'\n",
    "with open(dataset_pickle, 'rb') as f:\n",
    "    files = pickle.load(f)\n",
    "\n",
    "train_list, val_list, test_list = [], [], []\n",
    "\n",
    "for file in tqdm(files['train']):\n",
    "    seqs = joblib.load(file)\n",
    "    name = file.split('/')[-1]\n",
    "\n",
    "    for idx, seq in enumerate(seqs):\n",
    "        if len(seq) == 0:\n",
    "            continue\n",
    "        train_list.append(seq)\n",
    "        \n",
    "for file in tqdm(files['val']):\n",
    "    seqs = joblib.load(file)\n",
    "    name = file.split('/')[-1]\n",
    "    for idx, seq in enumerate(seqs):\n",
    "        if len(seq) == 0:\n",
    "            continue\n",
    "        val_list.append(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train length: 293775\n",
      "Val length: 37065\n",
      "Test length: 0\n"
     ]
    }
   ],
   "source": [
    "print(f'Train length: {len(train_list)}')\n",
    "print(f'Val length: {len(val_list)}')\n",
    "print(f'Test length: {len(test_list)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions / classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cpu_device():\n",
    "\n",
    "    return TORCH_CPU_DEVICE\n",
    "\n",
    "def get_device():\n",
    "\n",
    "    if((not USE_CUDA) or (TORCH_CUDA_DEVICE is None)):\n",
    "        return TORCH_CPU_DEVICE\n",
    "    else:\n",
    "        return TORCH_CUDA_DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EPianoDataset(Dataset):\n",
    "    \"\"\"\n",
    "    ----------\n",
    "    Author: Damon Gwinn\n",
    "    ----------\n",
    "    Pytorch Dataset for the Maestro e-piano dataset (https://magenta.tensorflow.org/datasets/maestro).\n",
    "    Recommended to use with Dataloader (https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)\n",
    "    Uses all files found in the given root directory of pre-processed (preprocess_midi.py)\n",
    "    Maestro midi files.\n",
    "    ----------\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, midi_list, max_seq=2048, random_seq=True):\n",
    "        self.max_seq    = max_seq\n",
    "        self.random_seq = random_seq\n",
    "        self.data_files = midi_list\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        ----------\n",
    "        Author: Damon Gwinn\n",
    "        ----------\n",
    "        How many data files exist in the given directory\n",
    "        ----------\n",
    "        \"\"\"\n",
    "\n",
    "        return len(self.data_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        ----------\n",
    "        Author: Damon Gwinn\n",
    "        ----------\n",
    "        Gets the indexed midi batch. Gets random sequence or from start depending on random_seq.\n",
    "        Returns the input and the target.\n",
    "        ----------\n",
    "        \"\"\"\n",
    "\n",
    "        raw_mid = torch.tensor(self.data_files[idx], dtype=TORCH_LABEL_TYPE, device=cpu_device())\n",
    "        x, tgt = process_midi(raw_mid, self.max_seq, self.random_seq)\n",
    "\n",
    "        return x, tgt\n",
    "    \n",
    "def process_midi(raw_mid, max_seq, random_seq):\n",
    "    \"\"\"\n",
    "    ----------\n",
    "    Author: Damon Gwinn\n",
    "    ----------\n",
    "    Takes in pre-processed raw midi and returns the input and target. Can use a random sequence or\n",
    "    go from the start based on random_seq.\n",
    "    ----------\n",
    "    \"\"\"\n",
    "\n",
    "    x   = torch.full((max_seq, ), TOKEN_PAD, dtype=TORCH_LABEL_TYPE, device=cpu_device())\n",
    "    tgt = torch.full((max_seq, ), TOKEN_PAD, dtype=TORCH_LABEL_TYPE, device=cpu_device())\n",
    "\n",
    "    raw_len     = len(raw_mid)\n",
    "    full_seq    = max_seq + 1 # Performing seq2seq\n",
    "\n",
    "    if(raw_len == 0):\n",
    "        return x, tgt\n",
    "\n",
    "    if(raw_len < full_seq):\n",
    "        if tgt.shape[0] == raw_len:\n",
    "            #print(f'Tgt shape: {tgt.shape} Raw len: {raw_len} Skipping')\n",
    "            x[:raw_len]         = raw_mid\n",
    "            tgt[:raw_len-1]     = raw_mid[1:]\n",
    "            tgt[raw_len-1]        = TOKEN_END\n",
    "        else:\n",
    "            x[:raw_len]         = raw_mid\n",
    "            tgt[:raw_len-1]     = raw_mid[1:]\n",
    "            tgt[raw_len]        = TOKEN_END\n",
    "    else:\n",
    "        # Randomly selecting a range\n",
    "        if(random_seq):\n",
    "            end_range = raw_len - full_seq\n",
    "            start = random.randint(SEQUENCE_START, end_range)\n",
    "\n",
    "        # Always taking from the start to as far as we can\n",
    "        else:\n",
    "            start = SEQUENCE_START\n",
    "\n",
    "        end = start + full_seq\n",
    "\n",
    "        data = raw_mid[start:end]\n",
    "\n",
    "        x = data[:max_seq]\n",
    "        tgt = data[1:full_seq]\n",
    "\n",
    "    return x, tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(cur_epoch, model, dataloader, loss, opt, lr_scheduler=None, num_iters=-1):\n",
    "    best_eval_acc        = 0.0\n",
    "    best_eval_acc_epoch  = -1\n",
    "    best_eval_loss       = float(\"inf\")\n",
    "    best_eval_loss_epoch = -1\n",
    "    loss_hist = []\n",
    "    \n",
    "    out = -1\n",
    "    model.train()\n",
    "    with tqdm(total=len(dataloader)) as bar_train:\n",
    "        for batch_num, batch in enumerate(dataloader):\n",
    "            time_before = time.time()\n",
    "\n",
    "            opt.zero_grad()\n",
    "\n",
    "            x   = batch[0].to(get_device())\n",
    "            tgt = batch[1].to(get_device())\n",
    "\n",
    "            y, _ = model(x)\n",
    "\n",
    "            y   = y.reshape(y.shape[0] * y.shape[1], -1)\n",
    "            tgt = tgt.flatten()\n",
    "\n",
    "            out = loss.forward(y, tgt)\n",
    "\n",
    "            out.backward()\n",
    "            opt.step()\n",
    "\n",
    "            if(lr_scheduler is not None):\n",
    "                lr_scheduler.step()\n",
    "\n",
    "            time_after = time.time()\n",
    "            time_took = time_after - time_before\n",
    "            lr = opt.param_groups[0]['lr']\n",
    "            bar_train.set_description(f'Epoch: {cur_epoch} Loss: {float(out):.4} LR: {float(lr):.8}')\n",
    "            bar_train.update(1)\n",
    "            loss_hist.append(out.item())\n",
    "            if batch_num == num_iters:\n",
    "                break\n",
    "\n",
    "    return loss_hist\n",
    "\n",
    "def compute_epiano_accuracy(out, tgt):\n",
    "    softmax = nn.Softmax(dim=-1)\n",
    "    out = torch.argmax(softmax(out), dim=-1)\n",
    "\n",
    "    out = out.flatten()\n",
    "    tgt = tgt.flatten()\n",
    "\n",
    "    mask = (tgt != TOKEN_PAD)\n",
    "\n",
    "    out = out[mask]\n",
    "    tgt = tgt[mask]\n",
    "\n",
    "    if(len(tgt) == 0):\n",
    "        return 1.0\n",
    "\n",
    "    num_right = (out == tgt)\n",
    "    num_right = torch.sum(num_right).type(TORCH_FLOAT)\n",
    "\n",
    "    acc = num_right / len(tgt)\n",
    "\n",
    "    return acc\n",
    "\n",
    "def eval_model(model, dataloader, loss, num_iters=-1):\n",
    "    model.eval()\n",
    "\n",
    "    avg_acc     = -1\n",
    "    avg_loss    = -1\n",
    "    with torch.set_grad_enabled(False):\n",
    "        n_test      = len(dataloader)\n",
    "        sum_loss   = 0.0\n",
    "        sum_acc    = 0.0\n",
    "        with tqdm(total=len(dataloader)) as bar_eval:\n",
    "            for batch in dataloader:\n",
    "                x   = batch[0].to(get_device())\n",
    "                tgt = batch[1].to(get_device())\n",
    "\n",
    "                y, _ = model(x)\n",
    "\n",
    "                sum_acc += float(compute_epiano_accuracy(y, tgt))\n",
    "\n",
    "                y   = y.reshape(y.shape[0] * y.shape[1], -1)\n",
    "                tgt = tgt.flatten()\n",
    "\n",
    "                out = loss.forward(y, tgt)\n",
    "\n",
    "                sum_loss += float(out)\n",
    "                bar_eval.set_description(f'Loss val: {float(out):.4}  Acc: {float(sum_acc / (bar_eval.n + 1)):.4}')\n",
    "                bar_eval.update(1)\n",
    "                if bar_eval.n == num_iters:\n",
    "                    break\n",
    "\n",
    "        avg_loss    = sum_loss / n_test\n",
    "        avg_acc     = sum_acc / n_test\n",
    "\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "class LrStepTracker:\n",
    "\n",
    "    def __init__(self, model_dim=512, warmup_steps=4000, init_steps=0):\n",
    "        # Store Values\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.model_dim = model_dim\n",
    "        self.init_steps = init_steps\n",
    "\n",
    "        # Begin Calculations\n",
    "        self.invsqrt_dim = (1 / math.sqrt(model_dim))\n",
    "        self.invsqrt_warmup = (1 / (warmup_steps * math.sqrt(warmup_steps)))\n",
    "\n",
    "    # step\n",
    "    def step(self, step):\n",
    "\n",
    "        step += self.init_steps\n",
    "        if(step <= self.warmup_steps):\n",
    "            return self.invsqrt_dim * self.invsqrt_warmup * step\n",
    "        else:\n",
    "            invsqrt_step = (1 / math.sqrt(step))\n",
    "            return self.invsqrt_dim * invsqrt_step\n",
    "\n",
    "# get_lr\n",
    "def get_lr(optimizer):\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_modulus = 1\n",
    "print_modulus = 1\n",
    "n_workers = 1\n",
    "\n",
    "lr = None\n",
    "ce_smoothing = None\n",
    "batch_size = 2\n",
    "random_seq = True\n",
    "epochs = 100\n",
    "\n",
    "rpr = False #'store_true'\n",
    "max_seq = 2048\n",
    "n_layers = 6\n",
    "num_heads = 8\n",
    "d_model = 512\n",
    "dim_feedforward = 1024\n",
    "dropout_prob = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = EPianoDataset(train_list, max_seq, random_seq)\n",
    "val_dataset = EPianoDataset(val_list, max_seq, random_seq)\n",
    "test_dataset = EPianoDataset(test_list, max_seq, random_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=n_workers, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=n_workers)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=n_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A vanilla multi-head masked self-attention layer with a projection at the end.\n",
    "    It is possible to use torch.nn.MultiheadAttention here but I am including an\n",
    "    explicit implementation here to show that there is nothing too scary here.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads\n",
    "        self.key = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.query = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.value = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # regularization\n",
    "        self.attn_drop = nn.Dropout(config.attn_pdrop)\n",
    "        self.resid_drop = nn.Dropout(config.resid_pdrop)\n",
    "        # output projection\n",
    "        self.proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                     .view(1, 1, config.block_size, config.block_size))\n",
    "        self.n_head = config.n_head\n",
    "\n",
    "    def forward(self, x, layer_past=None):\n",
    "        B, T, C = x.size()\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        k = self.key(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = self.query(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = self.value(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.mask[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_drop(att)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_drop(self.proj(y))\n",
    "        return y\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" an unassuming Transformer block \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
    "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
    "        self.enable_rpr = config.enable_rpr\n",
    "        if config.enable_rpr:\n",
    "            self.attn = MultiheadAttentionRPR(config.n_embd, config.n_head, config.attn_pdrop, er_len=config.er_len)\n",
    "        else:\n",
    "            self.attn = CausalSelfAttention(config)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(config.n_embd, config.dim_feedforward),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(config.dim_feedforward, config.n_embd),\n",
    "            nn.Dropout(config.resid_pdrop),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        if self.enable_rpr:\n",
    "            x = x + self.attn(self.ln1(x), self.ln1(x), self.ln1(x), attn_mask=mask)[0]\n",
    "        else:\n",
    "            x = x + self.attn(self.ln1(x)) \n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class MultiheadAttentionRPR(nn.Module):\n",
    "    \"\"\"\n",
    "    ----------\n",
    "    Author: Pytorch\n",
    "    Modified: Damon Gwinn\n",
    "    ----------\n",
    "    For Relative Position Representation support (https://arxiv.org/abs/1803.02155)\n",
    "    https://pytorch.org/docs/1.2.0/_modules/torch/nn/modules/activation.html#MultiheadAttention\n",
    "    Modification to add RPR embedding Er and call custom multi_head_attention_forward_rpr\n",
    "    ----------\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0., bias=True, add_bias_kv=False,\n",
    "                 add_zero_attn=False, kdim=None, vdim=None, er_len=None):\n",
    "        super(MultiheadAttentionRPR, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.kdim = kdim if kdim is not None else embed_dim\n",
    "        self.vdim = vdim if vdim is not None else embed_dim\n",
    "        self._qkv_same_embed_dim = self.kdim == embed_dim and self.vdim == embed_dim\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.in_proj_weight = Parameter(torch.empty(3 * embed_dim, embed_dim))\n",
    "\n",
    "        if self._qkv_same_embed_dim is False:\n",
    "            self.q_proj_weight = Parameter(torch.Tensor(embed_dim, embed_dim))\n",
    "            self.k_proj_weight = Parameter(torch.Tensor(embed_dim, self.kdim))\n",
    "            self.v_proj_weight = Parameter(torch.Tensor(embed_dim, self.vdim))\n",
    "\n",
    "        if bias:\n",
    "            self.in_proj_bias = Parameter(torch.empty(3 * embed_dim))\n",
    "        else:\n",
    "            self.register_parameter('in_proj_bias', None)\n",
    "        self.out_proj = Linear(embed_dim, embed_dim, bias=bias)\n",
    "\n",
    "        if add_bias_kv:\n",
    "            self.bias_k = Parameter(torch.empty(1, 1, embed_dim))\n",
    "            self.bias_v = Parameter(torch.empty(1, 1, embed_dim))\n",
    "        else:\n",
    "            self.bias_k = self.bias_v = None\n",
    "\n",
    "        self.add_zero_attn = add_zero_attn\n",
    "\n",
    "        # Adding RPR embedding matrix\n",
    "        if(er_len is not None):\n",
    "            self.Er = Parameter(torch.rand((er_len, self.head_dim), dtype=torch.float32))\n",
    "        else:\n",
    "            self.Er = None\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        if self._qkv_same_embed_dim:\n",
    "            xavier_uniform_(self.in_proj_weight)\n",
    "        else:\n",
    "            xavier_uniform_(self.q_proj_weight)\n",
    "            xavier_uniform_(self.k_proj_weight)\n",
    "            xavier_uniform_(self.v_proj_weight)\n",
    "\n",
    "        if self.in_proj_bias is not None:\n",
    "            constant_(self.in_proj_bias, 0.)\n",
    "            constant_(self.out_proj.bias, 0.)\n",
    "        if self.bias_k is not None:\n",
    "            xavier_normal_(self.bias_k)\n",
    "        if self.bias_v is not None:\n",
    "            xavier_normal_(self.bias_v)\n",
    "\n",
    "    def forward(self, query, key, value, key_padding_mask=None,\n",
    "                need_weights=True, attn_mask=None):\n",
    "\n",
    "        if hasattr(self, '_qkv_same_embed_dim') and self._qkv_same_embed_dim is False:\n",
    "            # return F.multi_head_attention_forward(\n",
    "            #     query, key, value, self.embed_dim, self.num_heads,\n",
    "            #     self.in_proj_weight, self.in_proj_bias,\n",
    "            #     self.bias_k, self.bias_v, self.add_zero_attn,\n",
    "            #     self.dropout, self.out_proj.weight, self.out_proj.bias,\n",
    "            #     training=self.training,\n",
    "            #     key_padding_mask=key_padding_mask, need_weights=need_weights,\n",
    "            #     attn_mask=attn_mask, use_separate_proj_weight=True,\n",
    "            #     q_proj_weight=self.q_proj_weight, k_proj_weight=self.k_proj_weight,\n",
    "            #     v_proj_weight=self.v_proj_weight)\n",
    "\n",
    "            return multi_head_attention_forward_rpr(\n",
    "                query, key, value, self.embed_dim, self.num_heads,\n",
    "                self.in_proj_weight, self.in_proj_bias,\n",
    "                self.bias_k, self.bias_v, self.add_zero_attn,\n",
    "                self.dropout, self.out_proj.weight, self.out_proj.bias,\n",
    "                training=self.training,\n",
    "                key_padding_mask=key_padding_mask, need_weights=need_weights,\n",
    "                attn_mask=attn_mask, use_separate_proj_weight=True,\n",
    "                q_proj_weight=self.q_proj_weight, k_proj_weight=self.k_proj_weight,\n",
    "                v_proj_weight=self.v_proj_weight, rpr_mat=self.Er)\n",
    "        else:\n",
    "            if not hasattr(self, '_qkv_same_embed_dim'):\n",
    "                warnings.warn('A new version of MultiheadAttention module has been implemented. \\\n",
    "                    Please re-train your model with the new module',\n",
    "                              UserWarning)\n",
    "\n",
    "            # return F.multi_head_attention_forward(\n",
    "            #     query, key, value, self.embed_dim, self.num_heads,\n",
    "            #     self.in_proj_weight, self.in_proj_bias,\n",
    "            #     self.bias_k, self.bias_v, self.add_zero_attn,\n",
    "            #     self.dropout, self.out_proj.weight, self.out_proj.bias,\n",
    "            #     training=self.training,\n",
    "            #     key_padding_mask=key_padding_mask, need_weights=need_weights,\n",
    "            #     attn_mask=attn_mask)\n",
    "\n",
    "            return multi_head_attention_forward_rpr(\n",
    "                query, key, value, self.embed_dim, self.num_heads,\n",
    "                self.in_proj_weight, self.in_proj_bias,\n",
    "                self.bias_k, self.bias_v, self.add_zero_attn,\n",
    "                self.dropout, self.out_proj.weight, self.out_proj.bias,\n",
    "                training=self.training,\n",
    "                key_padding_mask=key_padding_mask, need_weights=need_weights,\n",
    "                attn_mask=attn_mask, rpr_mat=self.Er)\n",
    "\n",
    "# multi_head_attention_forward_rpr\n",
    "def multi_head_attention_forward_rpr(query,                       # type: Tensor\n",
    "                                 key,                             # type: Tensor\n",
    "                                 value,                           # type: Tensor\n",
    "                                 embed_dim_to_check,              # type: int\n",
    "                                 num_heads,                       # type: int\n",
    "                                 in_proj_weight,                  # type: Tensor\n",
    "                                 in_proj_bias,                    # type: Tensor\n",
    "                                 bias_k,                          # type: Optional[Tensor]\n",
    "                                 bias_v,                          # type: Optional[Tensor]\n",
    "                                 add_zero_attn,                   # type: bool\n",
    "                                 dropout_p,                       # type: float\n",
    "                                 out_proj_weight,                 # type: Tensor\n",
    "                                 out_proj_bias,                   # type: Tensor\n",
    "                                 training=True,                   # type: bool\n",
    "                                 key_padding_mask=None,           # type: Optional[Tensor]\n",
    "                                 need_weights=True,               # type: bool\n",
    "                                 attn_mask=None,                  # type: Optional[Tensor]\n",
    "                                 use_separate_proj_weight=False,  # type: bool\n",
    "                                 q_proj_weight=None,              # type: Optional[Tensor]\n",
    "                                 k_proj_weight=None,              # type: Optional[Tensor]\n",
    "                                 v_proj_weight=None,              # type: Optional[Tensor]\n",
    "                                 static_k=None,                   # type: Optional[Tensor]\n",
    "                                 static_v=None,                   # type: Optional[Tensor]\n",
    "                                 rpr_mat=None\n",
    "                                 ):\n",
    "    '''\n",
    "    print('Query: ', query.shape, 'Key: ', key.shape, 'Value: ', value.shape)\n",
    "    print('Equal: ', torch.equal(query, key) and torch.equal(key, value))\n",
    "    print('embed_dim_to_check: ', embed_dim_to_check)\n",
    "    print('num_heads:', num_heads)\n",
    "    print('in_proj_weight: ', in_proj_weight.shape)\n",
    "    print('in_proj_bias: ', in_proj_bias.shape)\n",
    "    print('bias_k:', bias_k, 'bias_v', bias_v)\n",
    "    print('add_zero_attn:', add_zero_attn)\n",
    "    print('dropout_p: ', dropout_p)\n",
    "    print('out_proj_weight: ', out_proj_weight.shape)\n",
    "    print('out_proj_bias:', out_proj_bias.shape)\n",
    "    print('training:', training)\n",
    "    print('need_weights:', need_weights)\n",
    "    print('use_separate_proj_weight:', use_separate_proj_weight)\n",
    "\n",
    "    print('key_padding_mask:', key_padding_mask)\n",
    "    print('attn_mask:', attn_mask.shape)\n",
    "    print('q_proj_weight:', q_proj_weight)\n",
    "    print('k_proj_weight:', k_proj_weight)\n",
    "    print('v_proj_weight:', v_proj_weight)\n",
    "    print('static_k:', static_k)\n",
    "    print('static_v:', static_v)\n",
    "    print('rpr_mat:', rpr_mat.shape)\n",
    "    '''\n",
    "    \"\"\"\n",
    "    ----------\n",
    "    Author: Pytorch\n",
    "    Modified: Damon Gwinn\n",
    "    ----------\n",
    "    For Relative Position Representation support (https://arxiv.org/abs/1803.02155)\n",
    "    https://pytorch.org/docs/1.2.0/_modules/torch/nn/functional.html\n",
    "    Modification to take RPR embedding matrix and perform skew optimized RPR (https://arxiv.org/abs/1809.04281)\n",
    "    ----------\n",
    "    \"\"\"\n",
    "\n",
    "    # type: (...) -> Tuple[Tensor, Optional[Tensor]]\n",
    "\n",
    "    qkv_same = torch.equal(query, key) and torch.equal(key, value)\n",
    "    kv_same = torch.equal(key, value)\n",
    "\n",
    "    tgt_len, bsz, embed_dim = query.size()\n",
    "    assert embed_dim == embed_dim_to_check\n",
    "    assert list(query.size()) == [tgt_len, bsz, embed_dim]\n",
    "    assert key.size() == value.size()\n",
    "\n",
    "    head_dim = embed_dim // num_heads\n",
    "    assert head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "    scaling = float(head_dim) ** -0.5\n",
    "\n",
    "    if use_separate_proj_weight is not True:\n",
    "        if qkv_same:\n",
    "            # self-attention\n",
    "            q, k, v = linear(query, in_proj_weight, in_proj_bias).chunk(3, dim=-1)\n",
    "\n",
    "        elif kv_same:\n",
    "            # encoder-decoder attention\n",
    "            # This is inline in_proj function with in_proj_weight and in_proj_bias\n",
    "            _b = in_proj_bias\n",
    "            _start = 0\n",
    "            _end = embed_dim\n",
    "            _w = in_proj_weight[_start:_end, :]\n",
    "            if _b is not None:\n",
    "                _b = _b[_start:_end]\n",
    "            q = linear(query, _w, _b)\n",
    "\n",
    "            if key is None:\n",
    "                assert value is None\n",
    "                k = None\n",
    "                v = None\n",
    "            else:\n",
    "\n",
    "                # This is inline in_proj function with in_proj_weight and in_proj_bias\n",
    "                _b = in_proj_bias\n",
    "                _start = embed_dim\n",
    "                _end = None\n",
    "                _w = in_proj_weight[_start:, :]\n",
    "                if _b is not None:\n",
    "                    _b = _b[_start:]\n",
    "                k, v = linear(key, _w, _b).chunk(2, dim=-1)\n",
    "\n",
    "        else:\n",
    "            # This is inline in_proj function with in_proj_weight and in_proj_bias\n",
    "            _b = in_proj_bias\n",
    "            _start = 0\n",
    "            _end = embed_dim\n",
    "            _w = in_proj_weight[_start:_end, :]\n",
    "            if _b is not None:\n",
    "                _b = _b[_start:_end]\n",
    "            q = linear(query, _w, _b)\n",
    "\n",
    "            # This is inline in_proj function with in_proj_weight and in_proj_bias\n",
    "            _b = in_proj_bias\n",
    "            _start = embed_dim\n",
    "            _end = embed_dim * 2\n",
    "            _w = in_proj_weight[_start:_end, :]\n",
    "            if _b is not None:\n",
    "                _b = _b[_start:_end]\n",
    "            k = linear(key, _w, _b)\n",
    "\n",
    "            # This is inline in_proj function with in_proj_weight and in_proj_bias\n",
    "            _b = in_proj_bias\n",
    "            _start = embed_dim * 2\n",
    "            _end = None\n",
    "            _w = in_proj_weight[_start:, :]\n",
    "            if _b is not None:\n",
    "                _b = _b[_start:]\n",
    "            v = linear(value, _w, _b)\n",
    "    else:\n",
    "        q_proj_weight_non_opt = torch.jit._unwrap_optional(q_proj_weight)\n",
    "        len1, len2 = q_proj_weight_non_opt.size()\n",
    "        assert len1 == embed_dim and len2 == query.size(-1)\n",
    "\n",
    "        k_proj_weight_non_opt = torch.jit._unwrap_optional(k_proj_weight)\n",
    "        len1, len2 = k_proj_weight_non_opt.size()\n",
    "        assert len1 == embed_dim and len2 == key.size(-1)\n",
    "\n",
    "        v_proj_weight_non_opt = torch.jit._unwrap_optional(v_proj_weight)\n",
    "        len1, len2 = v_proj_weight_non_opt.size()\n",
    "        assert len1 == embed_dim and len2 == value.size(-1)\n",
    "\n",
    "        if in_proj_bias is not None:\n",
    "            q = linear(query, q_proj_weight_non_opt, in_proj_bias[0:embed_dim])\n",
    "            k = linear(key, k_proj_weight_non_opt, in_proj_bias[embed_dim:(embed_dim * 2)])\n",
    "            v = linear(value, v_proj_weight_non_opt, in_proj_bias[(embed_dim * 2):])\n",
    "        else:\n",
    "            q = linear(query, q_proj_weight_non_opt, in_proj_bias)\n",
    "            k = linear(key, k_proj_weight_non_opt, in_proj_bias)\n",
    "            v = linear(value, v_proj_weight_non_opt, in_proj_bias)\n",
    "    q = q * scaling\n",
    "\n",
    "    if bias_k is not None and bias_v is not None:\n",
    "        if static_k is None and static_v is None:\n",
    "            k = torch.cat([k, bias_k.repeat(1, bsz, 1)])\n",
    "            v = torch.cat([v, bias_v.repeat(1, bsz, 1)])\n",
    "            if attn_mask is not None:\n",
    "                attn_mask = torch.cat([attn_mask,\n",
    "                                      torch.zeros((attn_mask.size(0), 1),\n",
    "                                                  dtype=attn_mask.dtype,\n",
    "                                                  device=attn_mask.device)], dim=1)\n",
    "            if key_padding_mask is not None:\n",
    "                key_padding_mask = torch.cat(\n",
    "                    [key_padding_mask, torch.zeros((key_padding_mask.size(0), 1),\n",
    "                                                   dtype=key_padding_mask.dtype,\n",
    "                                                   device=key_padding_mask.device)], dim=1)\n",
    "        else:\n",
    "            assert static_k is None, \"bias cannot be added to static key.\"\n",
    "            assert static_v is None, \"bias cannot be added to static value.\"\n",
    "    else:\n",
    "        assert bias_k is None\n",
    "        assert bias_v is None\n",
    "\n",
    "    q = q.contiguous().view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    if k is not None:\n",
    "        k = k.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    if v is not None:\n",
    "        v = v.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "\n",
    "    if static_k is not None:\n",
    "        assert static_k.size(0) == bsz * num_heads\n",
    "        assert static_k.size(2) == head_dim\n",
    "        k = static_k\n",
    "\n",
    "    if static_v is not None:\n",
    "        assert static_v.size(0) == bsz * num_heads\n",
    "        assert static_v.size(2) == head_dim\n",
    "        v = static_v\n",
    "\n",
    "    src_len = k.size(1)\n",
    "\n",
    "    if key_padding_mask is not None:\n",
    "        assert key_padding_mask.size(0) == bsz\n",
    "        assert key_padding_mask.size(1) == src_len\n",
    "\n",
    "    if add_zero_attn:\n",
    "        src_len += 1\n",
    "        k = torch.cat([k, torch.zeros((k.size(0), 1) + k.size()[2:], dtype=k.dtype, device=k.device)], dim=1)\n",
    "        v = torch.cat([v, torch.zeros((v.size(0), 1) + v.size()[2:], dtype=v.dtype, device=v.device)], dim=1)\n",
    "        if attn_mask is not None:\n",
    "            attn_mask = torch.cat([attn_mask, torch.zeros((attn_mask.size(0), 1),\n",
    "                                                          dtype=attn_mask.dtype,\n",
    "                                                          device=attn_mask.device)], dim=1)\n",
    "        if key_padding_mask is not None:\n",
    "            key_padding_mask = torch.cat(\n",
    "                [key_padding_mask, torch.zeros((key_padding_mask.size(0), 1),\n",
    "                                               dtype=key_padding_mask.dtype,\n",
    "                                               device=key_padding_mask.device)], dim=1)\n",
    "\n",
    "    attn_output_weights = torch.bmm(q, k.transpose(1, 2))\n",
    "    assert list(attn_output_weights.size()) == [bsz * num_heads, tgt_len, src_len]\n",
    "\n",
    "    ######### ADDITION OF RPR ###########\n",
    "    if(rpr_mat is not None):\n",
    "        rpr_mat = _get_valid_embedding(rpr_mat, q.shape[1], k.shape[1])\n",
    "        qe = torch.einsum(\"hld,md->hlm\", q, rpr_mat)\n",
    "        srel = _skew(qe)\n",
    "\n",
    "        attn_output_weights += srel\n",
    "\n",
    "    if attn_mask is not None:\n",
    "        attn_mask = attn_mask.unsqueeze(0)\n",
    "        attn_output_weights += attn_mask\n",
    "\n",
    "    if key_padding_mask is not None:\n",
    "        attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)\n",
    "        attn_output_weights = attn_output_weights.masked_fill(\n",
    "            key_padding_mask.unsqueeze(1).unsqueeze(2),\n",
    "            float('-inf'),\n",
    "        )\n",
    "        attn_output_weights = attn_output_weights.view(bsz * num_heads, tgt_len, src_len)\n",
    "\n",
    "    attn_output_weights = softmax(\n",
    "        attn_output_weights, dim=-1)\n",
    "\n",
    "    attn_output_weights = dropout(attn_output_weights, p=dropout_p, training=training)\n",
    "\n",
    "    attn_output = torch.bmm(attn_output_weights, v)\n",
    "    assert list(attn_output.size()) == [bsz * num_heads, tgt_len, head_dim]\n",
    "    attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)\n",
    "    attn_output = linear(attn_output, out_proj_weight, out_proj_bias)\n",
    "\n",
    "    if need_weights:\n",
    "        # average attention weights over heads\n",
    "        attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)\n",
    "        return attn_output, attn_output_weights.sum(dim=1) / num_heads\n",
    "    else:\n",
    "        return attn_output, None\n",
    "\n",
    "def _get_valid_embedding(Er, len_q, len_k):\n",
    "    \"\"\"\n",
    "    ----------\n",
    "    Author: Damon Gwinn\n",
    "    ----------\n",
    "    Gets valid embeddings based on max length of RPR attention\n",
    "    ----------\n",
    "    \"\"\"\n",
    "\n",
    "    len_e = Er.shape[0]\n",
    "    start = max(0, len_e - len_q)\n",
    "    return Er[start:, :]\n",
    "\n",
    "def _skew(qe):\n",
    "    \"\"\"\n",
    "    ----------\n",
    "    Author: Damon Gwinn\n",
    "    ----------\n",
    "    Performs the skew optimized RPR computation (https://arxiv.org/abs/1809.04281)\n",
    "    ----------\n",
    "    \"\"\"\n",
    "\n",
    "    sz = qe.shape[1]\n",
    "    mask = (torch.triu(torch.ones(sz, sz).to(qe.device)) == 1).float().flip(0)\n",
    "\n",
    "    qe = mask * qe\n",
    "    qe = F.pad(qe, (1,0, 0,0, 0,0))\n",
    "    qe = torch.reshape(qe, (qe.shape[0], qe.shape[2], qe.shape[1]))\n",
    "\n",
    "    srel = qe[:, 1:, :]\n",
    "    return srel\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    \"\"\"  the full GPT language model, with a context size of block_size \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        # input embedding stem\n",
    "        self.tok_emb = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, config.block_size, config.n_embd))\n",
    "        self.drop = nn.Dropout(config.embd_pdrop)\n",
    "        # transformer\n",
    "        self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layer)])\n",
    "        # decoder head\n",
    "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
    "        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.enable_rpr = config.enable_rpr\n",
    "\n",
    "        self.block_size = config.block_size\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "        logger.info(\"number of parameters: %e\", sum(p.numel() for p in self.parameters()))\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.block_size\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def configure_optimizers(self, train_config):\n",
    "        \"\"\"\n",
    "        This long function is unfortunately doing something very simple and is being very defensive:\n",
    "        We are separating out all parameters of the model into two buckets: those that will experience\n",
    "        weight decay for regularization and those that won't (biases, and layernorm/embedding weights).\n",
    "        We are then returning the PyTorch optimizer object.\n",
    "        \"\"\"\n",
    "\n",
    "        # separate out all parameters to those that will and won't experience regularizing weight decay\n",
    "        decay = set()\n",
    "        no_decay = set()\n",
    "        whitelist_weight_modules = (torch.nn.Linear, )\n",
    "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
    "        for mn, m in self.named_modules():\n",
    "            for pn, p in m.named_parameters():\n",
    "                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n",
    "\n",
    "                if pn.endswith('bias'):\n",
    "                    # all biases will not be decayed\n",
    "                    no_decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
    "                    # weights of whitelist modules will be weight decayed\n",
    "                    decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
    "                    # weights of blacklist modules will NOT be weight decayed\n",
    "                    no_decay.add(fpn)\n",
    "\n",
    "        # special case the position embedding parameter in the root GPT module as not decayed\n",
    "        no_decay.add('pos_emb')\n",
    "\n",
    "        # validate that we considered every parameter\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        inter_params = decay & no_decay\n",
    "        union_params = decay | no_decay\n",
    "        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
    "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
    "                                                    % (str(param_dict.keys() - union_params), )\n",
    "\n",
    "        # create the pytorch optimizer object\n",
    "        optim_groups = [\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.weight_decay},\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=train_config.learning_rate, betas=train_config.betas)\n",
    "        return optimizer\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        b, t = idx.size()\n",
    "        if self.enable_rpr:\n",
    "            mask = generate_square_subsequent_mask(t).to(get_device())\n",
    "        else:\n",
    "            mask = None\n",
    "            \n",
    "        assert t <= self.block_size, \"Cannot forward, model block size is exhausted.\"\n",
    "\n",
    "        # forward the GPT model\n",
    "        token_embeddings = self.tok_emb(idx) # each index maps to a (learnable) vector\n",
    "        position_embeddings = self.pos_emb[:, :t, :] # each position maps to a (learnable) vector\n",
    "        x = self.drop(token_embeddings + position_embeddings)\n",
    "        if self.enable_rpr:\n",
    "            x = x.permute(1,0,2)\n",
    "            for module in self.blocks:\n",
    "                x = module(x, mask=mask)\n",
    "            x = x.permute(1,0,2)\n",
    "        else:\n",
    "            x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "\n",
    "        # if we are given some desired targets also calculate the loss\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        \n",
    "        if self.enable_rpr:\n",
    "            del mask\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, primer=None, target_seq_length=1024, beam=0, beam_chance=1.0):\n",
    "\n",
    "        assert (not self.training), \"Cannot generate while in training mode\"\n",
    "\n",
    "        print(\"Generating sequence of max length:\", target_seq_length)\n",
    "\n",
    "        gen_seq = torch.full((1,target_seq_length), TOKEN_PAD, dtype=TORCH_LABEL_TYPE, device=get_device())\n",
    "\n",
    "        num_primer = len(primer)\n",
    "        gen_seq[..., :num_primer] = primer.type(TORCH_LABEL_TYPE).to(get_device())\n",
    "\n",
    "        cur_i = num_primer\n",
    "        while(cur_i < target_seq_length):\n",
    "            logits, _ = self.forward(gen_seq[..., :cur_i])\n",
    "            y = self.softmax(logits)[..., :TOKEN_END]\n",
    "            token_probs = y[:, cur_i-1, :]\n",
    "\n",
    "            if(beam == 0):\n",
    "                beam_ran = 2.0\n",
    "            else:\n",
    "                beam_ran = random.uniform(0,1)\n",
    "\n",
    "            if(beam_ran <= beam_chance):\n",
    "                token_probs = token_probs.flatten()\n",
    "                top_res, top_i = torch.topk(token_probs, beam)\n",
    "\n",
    "                beam_rows = top_i // VOCAB_SIZE\n",
    "                beam_cols = top_i % VOCAB_SIZE\n",
    "\n",
    "                gen_seq = gen_seq[beam_rows, :]\n",
    "                gen_seq[..., cur_i] = beam_cols\n",
    "\n",
    "            else:\n",
    "                distrib = torch.distributions.categorical.Categorical(probs=token_probs)\n",
    "                next_token = distrib.sample()\n",
    "                gen_seq[:, cur_i] = next_token\n",
    "\n",
    "\n",
    "                # Let the transformer decide to end if it wants to\n",
    "                if(next_token == TOKEN_END):\n",
    "                    print(\"Model called end of sequence at:\", cur_i, \"/\", target_seq_length)\n",
    "                    break\n",
    "\n",
    "            cur_i += 1\n",
    "            if(cur_i % 50 == 0):\n",
    "                print(cur_i, \"/\", target_seq_length)\n",
    "\n",
    "        return gen_seq[:, :cur_i]\n",
    "    \n",
    "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "        r\"\"\"Generate a square mask for the sequence. The masked positions are filled with float('-inf').\n",
    "            Unmasked positions are filled with float(0.0).\n",
    "        \"\"\"\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "    \n",
    "class GPTConfig:\n",
    "    \"\"\" base GPT config, params common to all GPT versions \"\"\"\n",
    "    embd_pdrop = 0.1\n",
    "    resid_pdrop = 0.1\n",
    "    attn_pdrop = 0.1\n",
    "\n",
    "    def __init__(self, vocab_size, block_size, dim_feedforward, enable_rpr=False, er_len=None, **kwargs):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.enable_rpr = enable_rpr\n",
    "        self.er_len = er_len\n",
    "        for k,v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (tok_emb): Embedding(390, 512)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttentionRPR(\n",
       "        (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Block(\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttentionRPR(\n",
       "        (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): Block(\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttentionRPR(\n",
       "        (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): Block(\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttentionRPR(\n",
       "        (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): Block(\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttentionRPR(\n",
       "        (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): Block(\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttentionRPR(\n",
       "        (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (head): Linear(in_features=512, out_features=390, bias=False)\n",
       "  (softmax): Softmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = GPTConfig(VOCAB_SIZE, \n",
    "                   max_seq,\n",
    "                   dim_feedforward=dim_feedforward,\n",
    "                   n_layer=6, \n",
    "                   n_head=8, \n",
    "                   n_embd=512,\n",
    "                   enable_rpr=True,\n",
    "                   er_len=max_seq)\n",
    "model = GPT(config).to(get_device())\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_step = 0\n",
    "lr = LR_DEFAULT_START\n",
    "lr_stepper = LrStepTracker(d_model, SCHEDULER_WARMUP_STEPS, init_step)\n",
    "eval_loss_func = nn.CrossEntropyLoss(ignore_index=TOKEN_PAD)\n",
    "train_loss_func = eval_loss_func\n",
    "\n",
    "opt = Adam(model.parameters(), lr=lr, betas=(ADAM_BETA_1, ADAM_BETA_2), eps=ADAM_EPSILON)\n",
    "lr_scheduler = LambdaLR(opt, lr_stepper.step)\n",
    "\n",
    "n_workers = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=n_workers, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=n_workers)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=n_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 2.265 LR: 0.00042734106:   7%|▋         | 10695/146888 [55:25<11:35:11,  3.27it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch: 1 Loss: 2.176 LR: 0.00038550879:   9%|▉         | 13141/146888 [1:07:56<11:27:02,  3.24it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch: 1 Loss: 2.153 LR: 0.00035470164:  11%|█         | 15523/146888 [1:20:05<11:12:11,  3.26it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch: 1 Loss: 2.241 LR: 0.00033013838:  12%|█▏        | 17919/146888 [1:32:18<10:48:00,  3.32it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch: 1 Loss: 1.82 LR: 0.00030919384:  14%|█▍        | 20430/146888 [1:45:15<10:54:26,  3.22it/s] "
     ]
    }
   ],
   "source": [
    "best_eval_acc        = 0.0\n",
    "best_eval_acc_epoch  = -1\n",
    "best_eval_loss       = float(\"inf\")\n",
    "best_eval_loss_epoch = -1\n",
    "best_acc_file = '/home/storage/3020/db/K_cluster2_backup/TD/gpt2_rpr_acc_bsize2.pth'\n",
    "best_loss_file = '/home/storage/3020/db/K_cluster2_backup/TD/gpt2_rpr_loss_bsize2.pth'\n",
    "loss_train, loss_val, acc_val = [], [], []\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    new_best = False\n",
    "    \n",
    "    loss = train(epoch+1, model, train_loader, train_loss_func, opt, lr_scheduler, num_iters=-1)\n",
    "    loss_train.append(loss)\n",
    "    \n",
    "    eval_loss, eval_acc = eval_model(model, val_loader, eval_loss_func, num_iters=-1)\n",
    "    loss_val.append(eval_loss)\n",
    "    acc_val.append(eval_acc)\n",
    "    \n",
    "    if(eval_acc > best_eval_acc):\n",
    "        best_eval_acc = eval_acc\n",
    "        best_eval_acc_epoch  = epoch+1\n",
    "        torch.save(model.state_dict(), best_acc_file)\n",
    "        new_best = True\n",
    "\n",
    "    if(eval_loss < best_eval_loss):\n",
    "        best_eval_loss       = eval_loss\n",
    "        best_eval_loss_epoch = epoch+1\n",
    "        torch.save(model.state_dict(), best_loss_file)\n",
    "        new_best = True\n",
    "    \n",
    "    if(new_best):\n",
    "        print(\"Best eval acc epoch:\", best_eval_acc_epoch)\n",
    "        print(\"Best eval acc:\", best_eval_acc)\n",
    "        print(\"\")\n",
    "        print(\"Best eval loss epoch:\", best_eval_loss_epoch)\n",
    "        print(\"Best eval loss:\", best_eval_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
