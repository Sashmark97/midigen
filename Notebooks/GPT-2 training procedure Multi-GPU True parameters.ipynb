{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Имплементация модели и её обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.nn.modules.normalization import LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1020/1020 [13:11<00:00,  1.29it/s]\n",
      "100%|██████████| 131/131 [01:37<00:00,  1.34it/s]\n",
      "100%|██████████| 125/125 [01:39<00:00,  1.26it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_pickle = '/home/storage/3020/db/K_cluster2_backup/TD/data/data_split.pkl'\n",
    "with open(dataset_pickle, 'rb') as f:\n",
    "    files = pickle.load(f)\n",
    "\n",
    "train_list, val_list, test_list = [], [], []\n",
    "\n",
    "for file in tqdm(files['train']):\n",
    "    seqs = joblib.load(file)\n",
    "    name = file.split('/')[-1]\n",
    "\n",
    "    for idx, seq in enumerate(seqs):\n",
    "        if len(seq) == 0:\n",
    "            continue\n",
    "        train_list.append(seq)\n",
    "        \n",
    "for file in tqdm(files['val']):\n",
    "    seqs = joblib.load(file)\n",
    "    name = file.split('/')[-1]\n",
    "    for idx, seq in enumerate(seqs):\n",
    "        if len(seq) == 0:\n",
    "            continue\n",
    "        val_list.append(seq)\n",
    "    \n",
    "for file in tqdm(files['test']):\n",
    "    seqs = joblib.load(file)\n",
    "    for idx, seq in enumerate(seqs):\n",
    "        if len(seq) == 0:\n",
    "            continue\n",
    "        test_list.append(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train length: 293775\n",
      "Val length: 37065\n",
      "Test length: 34815\n"
     ]
    }
   ],
   "source": [
    "print(f'Train length: {len(train_list)}')\n",
    "print(f'Val length: {len(val_list)}')\n",
    "print(f'Test length: {len(test_list)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_START = 0\n",
    "RANGE_NOTE_ON = 128\n",
    "RANGE_NOTE_OFF = 128\n",
    "RANGE_VEL = 32\n",
    "RANGE_TIME_SHIFT = 100\n",
    "\n",
    "START_IDX = {\n",
    "    'note_on': 0,\n",
    "    'note_off': RANGE_NOTE_ON,\n",
    "    'time_shift': RANGE_NOTE_ON + RANGE_NOTE_OFF,\n",
    "    'velocity': RANGE_NOTE_ON + RANGE_NOTE_OFF + RANGE_TIME_SHIFT\n",
    "}\n",
    "\n",
    "\n",
    "SEPERATOR               = \"=========================\"\n",
    "\n",
    "# Taken from the paper\n",
    "ADAM_BETA_1             = 0.9\n",
    "ADAM_BETA_2             = 0.98\n",
    "ADAM_EPSILON            = 10e-9\n",
    "\n",
    "LR_DEFAULT_START        = 1.0\n",
    "SCHEDULER_WARMUP_STEPS  = 4000\n",
    "# LABEL_SMOOTHING_E       = 0.1\n",
    "\n",
    "# DROPOUT_P               = 0.1\n",
    "\n",
    "TOKEN_END               = RANGE_NOTE_ON + RANGE_NOTE_OFF + RANGE_VEL + RANGE_TIME_SHIFT\n",
    "TOKEN_PAD               = TOKEN_END + 1\n",
    "\n",
    "VOCAB_SIZE              = TOKEN_PAD + 1\n",
    "\n",
    "TORCH_FLOAT             = torch.float32\n",
    "TORCH_INT               = torch.int32\n",
    "\n",
    "TORCH_LABEL_TYPE        = torch.long\n",
    "\n",
    "PREPEND_ZEROS_WIDTH     = 4\n",
    "\n",
    "TORCH_CPU_DEVICE = torch.device(\"cpu\")\n",
    "USE_CUDA = 1\n",
    "TORCH_CUDA_DEVICE = torch.device(\"cuda:8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cpu_device():\n",
    "    \"\"\"\n",
    "    ----------\n",
    "    Author: Damon Gwinn\n",
    "    ----------\n",
    "    Grabs the cpu device\n",
    "    ----------\n",
    "    \"\"\"\n",
    "\n",
    "    return TORCH_CPU_DEVICE\n",
    "\n",
    "def get_device():\n",
    "    \"\"\"\n",
    "    ----------\n",
    "    Author: Damon Gwinn\n",
    "    ----------\n",
    "    Grabs the default device. Default device is CUDA if available and use_cuda is not False, CPU otherwise.\n",
    "    ----------\n",
    "    \"\"\"\n",
    "\n",
    "    if((not USE_CUDA) or (TORCH_CUDA_DEVICE is None)):\n",
    "        return TORCH_CPU_DEVICE\n",
    "    else:\n",
    "        return TORCH_CUDA_DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EPianoDataset(Dataset):\n",
    "    \"\"\"\n",
    "    ----------\n",
    "    Author: Damon Gwinn\n",
    "    ----------\n",
    "    Pytorch Dataset for the Maestro e-piano dataset (https://magenta.tensorflow.org/datasets/maestro).\n",
    "    Recommended to use with Dataloader (https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)\n",
    "    Uses all files found in the given root directory of pre-processed (preprocess_midi.py)\n",
    "    Maestro midi files.\n",
    "    ----------\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, midi_list, max_seq=2048, random_seq=True):\n",
    "        self.max_seq    = max_seq\n",
    "        self.random_seq = random_seq\n",
    "        self.data_files = midi_list\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        ----------\n",
    "        Author: Damon Gwinn\n",
    "        ----------\n",
    "        How many data files exist in the given directory\n",
    "        ----------\n",
    "        \"\"\"\n",
    "\n",
    "        return len(self.data_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        ----------\n",
    "        Author: Damon Gwinn\n",
    "        ----------\n",
    "        Gets the indexed midi batch. Gets random sequence or from start depending on random_seq.\n",
    "        Returns the input and the target.\n",
    "        ----------\n",
    "        \"\"\"\n",
    "\n",
    "        raw_mid = torch.tensor(self.data_files[idx], dtype=TORCH_LABEL_TYPE, device=cpu_device())\n",
    "        x, tgt = process_midi(raw_mid, self.max_seq, self.random_seq)\n",
    "\n",
    "        return x, tgt\n",
    "    \n",
    "def process_midi(raw_mid, max_seq, random_seq):\n",
    "    \"\"\"\n",
    "    ----------\n",
    "    Author: Damon Gwinn\n",
    "    ----------\n",
    "    Takes in pre-processed raw midi and returns the input and target. Can use a random sequence or\n",
    "    go from the start based on random_seq.\n",
    "    ----------\n",
    "    \"\"\"\n",
    "\n",
    "    x   = torch.full((max_seq, ), TOKEN_PAD, dtype=TORCH_LABEL_TYPE, device=cpu_device())\n",
    "    tgt = torch.full((max_seq, ), TOKEN_PAD, dtype=TORCH_LABEL_TYPE, device=cpu_device())\n",
    "\n",
    "    raw_len     = len(raw_mid)\n",
    "    full_seq    = max_seq + 1 # Performing seq2seq\n",
    "\n",
    "    if(raw_len == 0):\n",
    "        return x, tgt\n",
    "\n",
    "    if(raw_len < full_seq):\n",
    "        if tgt.shape[0] == raw_len:\n",
    "            #print(f'Tgt shape: {tgt.shape} Raw len: {raw_len} Skipping')\n",
    "            x[:raw_len]         = raw_mid\n",
    "            tgt[:raw_len-1]     = raw_mid[1:]\n",
    "            tgt[raw_len-1]        = TOKEN_END\n",
    "        else:\n",
    "            x[:raw_len]         = raw_mid\n",
    "            tgt[:raw_len-1]     = raw_mid[1:]\n",
    "            tgt[raw_len]        = TOKEN_END\n",
    "    else:\n",
    "        # Randomly selecting a range\n",
    "        if(random_seq):\n",
    "            end_range = raw_len - full_seq\n",
    "            start = random.randint(SEQUENCE_START, end_range)\n",
    "\n",
    "        # Always taking from the start to as far as we can\n",
    "        else:\n",
    "            start = SEQUENCE_START\n",
    "\n",
    "        end = start + full_seq\n",
    "\n",
    "        data = raw_mid[start:end]\n",
    "\n",
    "        x = data[:max_seq]\n",
    "        tgt = data[1:full_seq]\n",
    "\n",
    "    return x, tgt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Гиперпараметры обучения и параметры модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_modulus = 1\n",
    "print_modulus = 1\n",
    "n_workers = 1\n",
    "\n",
    "lr = None\n",
    "ce_smoothing = None\n",
    "batch_size = 12\n",
    "random_seq = True\n",
    "epochs = 100\n",
    "\n",
    "max_seq = 2048\n",
    "n_layers = 6\n",
    "num_heads = 8\n",
    "d_model = 512\n",
    "dim_feedforward = 1024\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Инициализируем датасеты собранными списками последовательностей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = EPianoDataset(train_list, max_seq, random_seq)\n",
    "val_dataset = EPianoDataset(val_list, max_seq, random_seq)\n",
    "test_dataset = EPianoDataset(test_list, max_seq, random_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=n_workers, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=n_workers)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=n_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим выходные шейпы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader\n",
      "X shape: torch.Size([8, 2048])\n",
      "Target shape: torch.Size([8, 2048])\n",
      "Validation loader\n",
      "X shape: torch.Size([8, 2048])\n",
      "Target shape: torch.Size([8, 2048])\n",
      "Test loader\n",
      "X shape: torch.Size([8, 2048])\n",
      "Target shape: torch.Size([8, 2048])\n"
     ]
    }
   ],
   "source": [
    "print('Train loader')\n",
    "for x, tgt in train_loader:\n",
    "    print(f'X shape: {x.shape}')\n",
    "    print(f'Target shape: {tgt.shape}')\n",
    "    break\n",
    "\n",
    "print('Validation loader')\n",
    "for x, tgt in val_loader:\n",
    "    print(f'X shape: {x.shape}')\n",
    "    print(f'Target shape: {tgt.shape}')\n",
    "    break\n",
    "\n",
    "print('Test loader')\n",
    "for x, tgt in test_loader:\n",
    "    print(f'X shape: {x.shape}')\n",
    "    print(f'Target shape: {tgt.shape}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Инициализация модели, оптимизатора и schedulera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LrStepTracker:\n",
    "\n",
    "    def __init__(self, model_dim=512, warmup_steps=4000, init_steps=0):\n",
    "        # Store Values\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.model_dim = model_dim\n",
    "        self.init_steps = init_steps\n",
    "\n",
    "        # Begin Calculations\n",
    "        self.invsqrt_dim = (1 / math.sqrt(model_dim))\n",
    "        self.invsqrt_warmup = (1 / (warmup_steps * math.sqrt(warmup_steps)))\n",
    "\n",
    "    # step\n",
    "    def step(self, step):\n",
    "\n",
    "        step += self.init_steps\n",
    "        if(step <= self.warmup_steps):\n",
    "            return self.invsqrt_dim * self.invsqrt_warmup * step\n",
    "        else:\n",
    "            invsqrt_step = (1 / math.sqrt(step))\n",
    "            return self.invsqrt_dim * invsqrt_step\n",
    "\n",
    "# get_lr\n",
    "def get_lr(optimizer):\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(cur_epoch, model, dataloader, loss, opt, lr_scheduler=None, num_iters=-1):\n",
    "    best_eval_acc        = 0.0\n",
    "    best_eval_acc_epoch  = -1\n",
    "    best_eval_loss       = float(\"inf\")\n",
    "    best_eval_loss_epoch = -1\n",
    "    loss_hist = []\n",
    "    \n",
    "    out = -1\n",
    "    model.train()\n",
    "    with tqdm(total=len(dataloader)) as bar_train:\n",
    "        for batch_num, batch in enumerate(dataloader):\n",
    "            time_before = time.time()\n",
    "\n",
    "            opt.zero_grad()\n",
    "\n",
    "            x   = batch[0].to(get_device())\n",
    "            tgt = batch[1].to(get_device())\n",
    "\n",
    "            y, _ = model(x)\n",
    "\n",
    "            y   = y.reshape(y.shape[0] * y.shape[1], -1)\n",
    "            tgt = tgt.flatten()\n",
    "\n",
    "            out = loss.forward(y, tgt)\n",
    "\n",
    "            out.backward()\n",
    "            opt.step()\n",
    "\n",
    "            if(lr_scheduler is not None):\n",
    "                lr_scheduler.step()\n",
    "\n",
    "            time_after = time.time()\n",
    "            time_took = time_after - time_before\n",
    "            lr = opt.param_groups[0]['lr']\n",
    "            bar_train.set_description(f'Epoch: {cur_epoch} Loss: {float(out):.4} LR: {float(lr):.8}')\n",
    "            bar_train.update(1)\n",
    "            loss_hist.append(out.item())\n",
    "            if batch_num == num_iters:\n",
    "                break\n",
    "\n",
    "    return loss_hist\n",
    "\n",
    "def compute_epiano_accuracy(out, tgt):\n",
    "    softmax = nn.Softmax(dim=-1)\n",
    "    out = torch.argmax(softmax(out), dim=-1)\n",
    "\n",
    "    out = out.flatten()\n",
    "    tgt = tgt.flatten()\n",
    "\n",
    "    mask = (tgt != TOKEN_PAD)\n",
    "\n",
    "    out = out[mask]\n",
    "    tgt = tgt[mask]\n",
    "\n",
    "    if(len(tgt) == 0):\n",
    "        return 1.0\n",
    "\n",
    "    num_right = (out == tgt)\n",
    "    num_right = torch.sum(num_right).type(TORCH_FLOAT)\n",
    "\n",
    "    acc = num_right / len(tgt)\n",
    "\n",
    "    return acc\n",
    "\n",
    "def eval_model(model, dataloader, loss, num_iters=-1):\n",
    "    model.eval()\n",
    "\n",
    "    avg_acc     = -1\n",
    "    avg_loss    = -1\n",
    "    with torch.set_grad_enabled(False):\n",
    "        n_test      = len(dataloader)\n",
    "        sum_loss   = 0.0\n",
    "        sum_acc    = 0.0\n",
    "        with tqdm(total=len(dataloader)) as bar_eval:\n",
    "            for batch in dataloader:\n",
    "                x   = batch[0].to(get_device())\n",
    "                tgt = batch[1].to(get_device())\n",
    "\n",
    "                y, _ = model(x)\n",
    "\n",
    "                sum_acc += float(compute_epiano_accuracy(y, tgt))\n",
    "\n",
    "                y   = y.reshape(y.shape[0] * y.shape[1], -1)\n",
    "                tgt = tgt.flatten()\n",
    "\n",
    "                out = loss.forward(y, tgt)\n",
    "\n",
    "                sum_loss += float(out)\n",
    "                bar_eval.set_description(f'Loss val: {float(out):.4}  Acc: {float(sum_acc / (bar_eval.n + 1)):.4}')\n",
    "                bar_eval.update(1)\n",
    "                if bar_eval.n == num_iters:\n",
    "                    break\n",
    "\n",
    "        avg_loss    = sum_loss / n_test\n",
    "        avg_acc     = sum_acc / n_test\n",
    "\n",
    "    return avg_loss, avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A vanilla multi-head masked self-attention layer with a projection at the end.\n",
    "    It is possible to use torch.nn.MultiheadAttention here but I am including an\n",
    "    explicit implementation here to show that there is nothing too scary here.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads\n",
    "        self.key = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.query = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.value = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # regularization\n",
    "        self.attn_drop = nn.Dropout(config.attn_pdrop)\n",
    "        self.resid_drop = nn.Dropout(config.resid_pdrop)\n",
    "        # output projection\n",
    "        self.proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                     .view(1, 1, config.block_size, config.block_size))\n",
    "        self.n_head = config.n_head\n",
    "\n",
    "    def forward(self, x, layer_past=None):\n",
    "        B, T, C = x.size()\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        k = self.key(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = self.query(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = self.value(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.mask[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_drop(att)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_drop(self.proj(y))\n",
    "        return y\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" an unassuming Transformer block \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
    "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(config.n_embd, config.dim_feedforward),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(config.dim_feedforward, config.n_embd),\n",
    "            nn.Dropout(config.resid_pdrop),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    \"\"\"  the full GPT language model, with a context size of block_size \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        # input embedding stem\n",
    "        self.tok_emb = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, config.block_size, config.n_embd))\n",
    "        self.drop = nn.Dropout(config.embd_pdrop)\n",
    "        # transformer\n",
    "        self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layer)])\n",
    "        # decoder head\n",
    "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
    "        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        self.block_size = config.block_size\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "        logger.info(\"number of parameters: %e\", sum(p.numel() for p in self.parameters()))\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.block_size\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def configure_optimizers(self, train_config):\n",
    "        \"\"\"\n",
    "        This long function is unfortunately doing something very simple and is being very defensive:\n",
    "        We are separating out all parameters of the model into two buckets: those that will experience\n",
    "        weight decay for regularization and those that won't (biases, and layernorm/embedding weights).\n",
    "        We are then returning the PyTorch optimizer object.\n",
    "        \"\"\"\n",
    "\n",
    "        # separate out all parameters to those that will and won't experience regularizing weight decay\n",
    "        decay = set()\n",
    "        no_decay = set()\n",
    "        whitelist_weight_modules = (torch.nn.Linear, )\n",
    "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
    "        for mn, m in self.named_modules():\n",
    "            for pn, p in m.named_parameters():\n",
    "                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n",
    "\n",
    "                if pn.endswith('bias'):\n",
    "                    # all biases will not be decayed\n",
    "                    no_decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
    "                    # weights of whitelist modules will be weight decayed\n",
    "                    decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
    "                    # weights of blacklist modules will NOT be weight decayed\n",
    "                    no_decay.add(fpn)\n",
    "\n",
    "        # special case the position embedding parameter in the root GPT module as not decayed\n",
    "        no_decay.add('pos_emb')\n",
    "\n",
    "        # validate that we considered every parameter\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        inter_params = decay & no_decay\n",
    "        union_params = decay | no_decay\n",
    "        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
    "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
    "                                                    % (str(param_dict.keys() - union_params), )\n",
    "\n",
    "        # create the pytorch optimizer object\n",
    "        optim_groups = [\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.weight_decay},\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=train_config.learning_rate, betas=train_config.betas)\n",
    "        return optimizer\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.block_size, \"Cannot forward, model block size is exhausted.\"\n",
    "\n",
    "        # forward the GPT model\n",
    "        token_embeddings = self.tok_emb(idx) # each index maps to a (learnable) vector\n",
    "        position_embeddings = self.pos_emb[:, :t, :] # each position maps to a (learnable) vector\n",
    "        x = self.drop(token_embeddings + position_embeddings)\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "\n",
    "        # if we are given some desired targets also calculate the loss\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, primer=None, target_seq_length=1024, beam=0, beam_chance=1.0):\n",
    "\n",
    "        assert (not self.training), \"Cannot generate while in training mode\"\n",
    "\n",
    "        print(\"Generating sequence of max length:\", target_seq_length)\n",
    "\n",
    "        gen_seq = torch.full((1,target_seq_length), TOKEN_PAD, dtype=TORCH_LABEL_TYPE, device=get_device())\n",
    "\n",
    "        num_primer = len(primer)\n",
    "        gen_seq[..., :num_primer] = primer.type(TORCH_LABEL_TYPE).to(get_device())\n",
    "\n",
    "        cur_i = num_primer\n",
    "        while(cur_i < target_seq_length):\n",
    "            logits, _ = self.forward(gen_seq[..., :cur_i])\n",
    "            y = self.softmax(logits)[..., :TOKEN_END]\n",
    "            token_probs = y[:, cur_i-1, :]\n",
    "\n",
    "            if(beam == 0):\n",
    "                beam_ran = 2.0\n",
    "            else:\n",
    "                beam_ran = random.uniform(0,1)\n",
    "\n",
    "            if(beam_ran <= beam_chance):\n",
    "                token_probs = token_probs.flatten()\n",
    "                top_res, top_i = torch.topk(token_probs, beam)\n",
    "\n",
    "                beam_rows = top_i // VOCAB_SIZE\n",
    "                beam_cols = top_i % VOCAB_SIZE\n",
    "\n",
    "                gen_seq = gen_seq[beam_rows, :]\n",
    "                gen_seq[..., cur_i] = beam_cols\n",
    "\n",
    "            else:\n",
    "                distrib = torch.distributions.categorical.Categorical(probs=token_probs)\n",
    "                next_token = distrib.sample()\n",
    "                gen_seq[:, cur_i] = next_token\n",
    "\n",
    "\n",
    "                # Let the transformer decide to end if it wants to\n",
    "                if(next_token == TOKEN_END):\n",
    "                    print(\"Model called end of sequence at:\", cur_i, \"/\", target_seq_length)\n",
    "                    break\n",
    "\n",
    "            cur_i += 1\n",
    "            if(cur_i % 50 == 0):\n",
    "                print(cur_i, \"/\", target_seq_length)\n",
    "\n",
    "        return gen_seq[:, :cur_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTConfig:\n",
    "    \"\"\" base GPT config, params common to all GPT versions \"\"\"\n",
    "    embd_pdrop = 0.1\n",
    "    resid_pdrop = 0.1\n",
    "    attn_pdrop = 0.1\n",
    "\n",
    "    def __init__(self, vocab_size, block_size, dim_feedforward, **kwargs):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        for k,v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (tok_emb): Embedding(390, 512)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): CausalSelfAttention(\n",
       "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Block(\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): CausalSelfAttention(\n",
       "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): Block(\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): CausalSelfAttention(\n",
       "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): Block(\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): CausalSelfAttention(\n",
       "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): Block(\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): CausalSelfAttention(\n",
       "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): Block(\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): CausalSelfAttention(\n",
       "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (head): Linear(in_features=512, out_features=390, bias=False)\n",
       "  (softmax): Softmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = GPTConfig(VOCAB_SIZE, \n",
    "                  max_seq,\n",
    "                  dim_feedforward=dim_feedforward,\n",
    "                  n_layer=6, \n",
    "                  n_head=8, \n",
    "                  n_embd=512)\n",
    "model = GPT(config).to(get_device())\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.DataParallel(model, device_ids=[8,7,9,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_step = 0\n",
    "lr = LR_DEFAULT_START\n",
    "lr_stepper = LrStepTracker(d_model, SCHEDULER_WARMUP_STEPS, init_step)\n",
    "eval_loss_func = nn.CrossEntropyLoss(ignore_index=TOKEN_PAD)\n",
    "train_loss_func = eval_loss_func\n",
    "\n",
    "opt = Adam(model.parameters(), lr=lr, betas=(ADAM_BETA_1, ADAM_BETA_2), eps=ADAM_EPSILON)\n",
    "lr_scheduler = LambdaLR(opt, lr_stepper.step)\n",
    "\n",
    "n_workers = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=n_workers, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=n_workers)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=n_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 1.879 LR: 0.00028245: 100%|██████████| 24482/24482 [2:19:22<00:00,  2.93it/s]      \n",
      "Loss val: 2.029  Acc: 0.3843: 100%|██████████| 3089/3089 [08:13<00:00,  6.25it/s] \n",
      "  0%|          | 0/24482 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best eval acc epoch: 1\n",
      "Best eval acc: 0.384279597428826\n",
      "\n",
      "Best eval loss epoch: 1\n",
      "Best eval loss: 2.054124534072209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Loss: 1.753 LR: 0.00019972231: 100%|██████████| 24482/24482 [2:18:36<00:00,  2.94it/s]  \n",
      "Loss val: 1.99  Acc: 0.3955: 100%|██████████| 3089/3089 [08:00<00:00,  6.43it/s]   \n",
      "  0%|          | 0/24482 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best eval acc epoch: 2\n",
      "Best eval acc: 0.3955075197975857\n",
      "\n",
      "Best eval loss epoch: 2\n",
      "Best eval loss: 1.994569742282103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Loss: 1.973 LR: 0.00017112309:  72%|███████▏  | 17734/24482 [1:41:24<35:34,  3.16it/s]   IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch: 3 Loss: 1.873 LR: 0.00016808243:  82%|████████▏ | 20169/24482 [1:55:26<22:49,  3.15it/s]  IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch: 3 Loss: 1.833 LR: 0.00016521446:  92%|█████████▏| 22589/24482 [2:09:30<09:49,  3.21it/s]  IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch: 3 Loss: 1.85 LR: 0.00016307258: 100%|██████████| 24482/24482 [2:20:23<00:00,  2.91it/s] \n",
      "Loss val: 1.961  Acc: 0.4011: 100%|██████████| 3089/3089 [07:39<00:00,  6.73it/s]  \n",
      "  0%|          | 0/24482 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best eval acc epoch: 3\n",
      "Best eval acc: 0.40110184190998033\n",
      "\n",
      "Best eval loss epoch: 3\n",
      "Best eval loss: 1.9650320271605926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Loss: 1.783 LR: 0.00016264574:   2%|▏         | 386/24482 [02:14<2:05:04,  3.21it/s] "
     ]
    }
   ],
   "source": [
    "best_eval_acc        = 0.0\n",
    "best_eval_acc_epoch  = -1\n",
    "best_eval_loss       = float(\"inf\")\n",
    "best_eval_loss_epoch = -1\n",
    "best_acc_file = '/home/storage/3020/db/K_cluster2_backup/TD/gpt2_best_acc_bsize12.pth'\n",
    "best_loss_file = '/home/storage/3020/db/K_cluster2_backup/TD/gpt2_best_loss_bsize12.pth'\n",
    "loss_train, loss_val, acc_val = [], [], []\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    new_best = False\n",
    "    \n",
    "    loss = train(epoch+1, model, train_loader, train_loss_func, opt, lr_scheduler, num_iters=-1)\n",
    "    loss_train.append(loss)\n",
    "    \n",
    "    eval_loss, eval_acc = eval_model(model, val_loader, eval_loss_func, num_iters=-1)\n",
    "    loss_val.append(eval_loss)\n",
    "    acc_val.append(eval_acc)\n",
    "    \n",
    "    if(eval_acc > best_eval_acc):\n",
    "        best_eval_acc = eval_acc\n",
    "        best_eval_acc_epoch  = epoch+1\n",
    "        torch.save(model.state_dict(), best_acc_file)\n",
    "        new_best = True\n",
    "\n",
    "    if(eval_loss < best_eval_loss):\n",
    "        best_eval_loss       = eval_loss\n",
    "        best_eval_loss_epoch = epoch+1\n",
    "        torch.save(model.state_dict(), best_loss_file)\n",
    "        new_best = True\n",
    "    \n",
    "    if(new_best):\n",
    "        print(\"Best eval acc epoch:\", best_eval_acc_epoch)\n",
    "        print(\"Best eval acc:\", best_eval_acc)\n",
    "        print(\"\")\n",
    "        print(\"Best eval loss epoch:\", best_eval_loss_epoch)\n",
    "        print(\"Best eval loss:\", best_eval_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f5560a46090>]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "tr_loss_list = [item for sublist in loss_train for item in sublist]\n",
    "plt.plot([i for i in range(len(tr_loss_list))] ,tr_loss_list, 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f558c04b090>]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([i for i in range(len(loss_val))], loss_val, 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f55870aaed0>]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([i for i in range(len(acc_val))], acc_val, 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dict = {'tr_loss': tr_loss_list,\n",
    "            'loss_val': loss_val,\n",
    "            'acc_val': acc_val,\n",
    "            'n_layers': 6,\n",
    "            'num_heads': 8,\n",
    "            'n_embd': 512,\n",
    "            'block_size': model.get_block_size(),\n",
    "            'dim_feedforward': 2048,\n",
    "            'dropout': dropout,\n",
    "            'max_seq': max_seq,\n",
    "            'VOCAB_SIZE': VOCAB_SIZE}\n",
    "with open('/home/storage/3020/db/K_cluster2_backup/TD/gpt2_best_acc_bsize2_scores_params.pkl', 'wb') as f:\n",
    "    pickle.dump(res_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(best_acc_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_block_size() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
