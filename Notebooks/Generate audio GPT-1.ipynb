{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pretty_midi\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.nn.modules.normalization import LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_pickle = '/home/storage/3020/db/K_cluster2_backup/TD/data/data_split.pkl'\n",
    "with open(dataset_pickle, 'rb') as f:\n",
    "    files = pickle.load(f)\n",
    "\n",
    "train_list, val_list, test_list = [], [], []\n",
    "\n",
    "for file in tqdm(files['train']):\n",
    "    seqs = joblib.load(file)\n",
    "    for idx, seq in enumerate(seqs):\n",
    "        if len(seq) == 0:\n",
    "            continue\n",
    "        train_list.append(seq)\n",
    "        \n",
    "for file in tqdm(files['val']):\n",
    "    seqs = joblib.load(file)\n",
    "    for idx, seq in enumerate(seqs):\n",
    "        if len(seq) == 0:\n",
    "            continue\n",
    "        val_list.append(seq)\n",
    "    \n",
    "for file in tqdm(files['test']):\n",
    "    seqs = joblib.load(file)\n",
    "    for idx, seq in enumerate(seqs):\n",
    "        if len(seq) == 0:\n",
    "            continue\n",
    "        test_list.append(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_START = 0\n",
    "RANGE_NOTE_ON = 128\n",
    "RANGE_NOTE_OFF = 128\n",
    "RANGE_VEL = 32\n",
    "RANGE_TIME_SHIFT = 100\n",
    "\n",
    "START_IDX = {\n",
    "    'note_on': 0,\n",
    "    'note_off': RANGE_NOTE_ON,\n",
    "    'time_shift': RANGE_NOTE_ON + RANGE_NOTE_OFF,\n",
    "    'velocity': RANGE_NOTE_ON + RANGE_NOTE_OFF + RANGE_TIME_SHIFT\n",
    "}\n",
    "\n",
    "\n",
    "SEPERATOR               = \"=========================\"\n",
    "\n",
    "# Taken from the paper\n",
    "ADAM_BETA_1             = 0.9\n",
    "ADAM_BETA_2             = 0.98\n",
    "ADAM_EPSILON            = 10e-9\n",
    "\n",
    "LR_DEFAULT_START        = 1.0\n",
    "SCHEDULER_WARMUP_STEPS  = 4000\n",
    "# LABEL_SMOOTHING_E       = 0.1\n",
    "\n",
    "# DROPOUT_P               = 0.1\n",
    "\n",
    "TOKEN_END               = RANGE_NOTE_ON + RANGE_NOTE_OFF + RANGE_VEL + RANGE_TIME_SHIFT\n",
    "TOKEN_PAD               = TOKEN_END + 1\n",
    "\n",
    "VOCAB_SIZE              = TOKEN_PAD + 1\n",
    "\n",
    "TORCH_FLOAT             = torch.float32\n",
    "TORCH_INT               = torch.int32\n",
    "\n",
    "TORCH_LABEL_TYPE        = torch.long\n",
    "\n",
    "PREPEND_ZEROS_WIDTH     = 4\n",
    "\n",
    "TORCH_CPU_DEVICE = torch.device(\"cpu\")\n",
    "USE_CUDA = 1\n",
    "TORCH_CUDA_DEVICE = torch.device(\"cuda:8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cpu_device():\n",
    "\n",
    "    return TORCH_CPU_DEVICE\n",
    "\n",
    "def get_device():\n",
    "    if((not USE_CUDA) or (TORCH_CUDA_DEVICE is None)):\n",
    "        return TORCH_CPU_DEVICE\n",
    "    else:\n",
    "        return TORCH_CUDA_DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-1 generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SustainAdapter:\n",
    "    def __init__(self, time, type):\n",
    "        self.start =  time\n",
    "        self.type = type\n",
    "\n",
    "\n",
    "class SustainDownManager:\n",
    "    def __init__(self, start, end):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.managed_notes = []\n",
    "        self._note_dict = {} # key: pitch, value: note.start\n",
    "\n",
    "    def add_managed_note(self, note: pretty_midi.Note):\n",
    "        self.managed_notes.append(note)\n",
    "\n",
    "    def transposition_notes(self):\n",
    "        for note in reversed(self.managed_notes):\n",
    "            try:\n",
    "                note.end = self._note_dict[note.pitch]\n",
    "            except KeyError:\n",
    "                note.end = max(self.end, note.end)\n",
    "            self._note_dict[note.pitch] = note.start\n",
    "\n",
    "\n",
    "# Divided note by note_on, note_off\n",
    "class SplitNote:\n",
    "    def __init__(self, type, time, value, velocity):\n",
    "        ## type: note_on, note_off\n",
    "        self.type = type\n",
    "        self.time = time\n",
    "        self.velocity = velocity\n",
    "        self.value = value\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '<[SNote] time: {} type: {}, value: {}, velocity: {}>'\\\n",
    "            .format(self.time, self.type, self.value, self.velocity)\n",
    "\n",
    "\n",
    "class Event:\n",
    "    def __init__(self, event_type, value):\n",
    "        self.type = event_type\n",
    "        self.value = value\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '<Event type: {}, value: {}>'.format(self.type, self.value)\n",
    "\n",
    "    def to_int(self):\n",
    "        return START_IDX[self.type] + self.value\n",
    "\n",
    "    @staticmethod\n",
    "    def from_int(int_value):\n",
    "        info = Event._type_check(int_value)\n",
    "        return Event(info['type'], info['value'])\n",
    "\n",
    "    @staticmethod\n",
    "    def _type_check(int_value):\n",
    "        range_note_on = range(0, RANGE_NOTE_ON)\n",
    "        range_note_off = range(RANGE_NOTE_ON, RANGE_NOTE_ON+RANGE_NOTE_OFF)\n",
    "        range_time_shift = range(RANGE_NOTE_ON+RANGE_NOTE_OFF,RANGE_NOTE_ON+RANGE_NOTE_OFF+RANGE_TIME_SHIFT)\n",
    "\n",
    "        valid_value = int_value\n",
    "\n",
    "        if int_value in range_note_on:\n",
    "            return {'type': 'note_on', 'value': valid_value}\n",
    "        elif int_value in range_note_off:\n",
    "            valid_value -= RANGE_NOTE_ON\n",
    "            return {'type': 'note_off', 'value': valid_value}\n",
    "        elif int_value in range_time_shift:\n",
    "            valid_value -= (RANGE_NOTE_ON + RANGE_NOTE_OFF)\n",
    "            return {'type': 'time_shift', 'value': valid_value}\n",
    "        else:\n",
    "            valid_value -= (RANGE_NOTE_ON + RANGE_NOTE_OFF + RANGE_TIME_SHIFT)\n",
    "            return {'type': 'velocity', 'value': valid_value}\n",
    "\n",
    "\n",
    "def _divide_note(notes):\n",
    "    result_array = []\n",
    "    notes.sort(key=lambda x: x.start)\n",
    "\n",
    "    for note in notes:\n",
    "        on = SplitNote('note_on', note.start, note.pitch, note.velocity)\n",
    "        off = SplitNote('note_off', note.end, note.pitch, None)\n",
    "        result_array += [on, off]\n",
    "    return result_array\n",
    "\n",
    "\n",
    "def _merge_note(snote_sequence):\n",
    "    note_on_dict = {}\n",
    "    result_array = []\n",
    "\n",
    "    for snote in snote_sequence:\n",
    "        # print(note_on_dict)\n",
    "        if snote.type == 'note_on':\n",
    "            note_on_dict[snote.value] = snote\n",
    "        elif snote.type == 'note_off':\n",
    "            try:\n",
    "                on = note_on_dict[snote.value]\n",
    "                off = snote\n",
    "                if off.time - on.time == 0:\n",
    "                    continue\n",
    "                result = pretty_midi.Note(on.velocity, snote.value, on.time, off.time)\n",
    "                result_array.append(result)\n",
    "            except:\n",
    "                print('info removed pitch: {}'.format(snote.value))\n",
    "    return result_array\n",
    "\n",
    "\n",
    "def _snote2events(snote: SplitNote, prev_vel: int):\n",
    "    result = []\n",
    "    if snote.velocity is not None:\n",
    "        modified_velocity = snote.velocity // 4\n",
    "        if prev_vel != modified_velocity:\n",
    "            result.append(Event(event_type='velocity', value=modified_velocity))\n",
    "    result.append(Event(event_type=snote.type, value=snote.value))\n",
    "    return result\n",
    "\n",
    "\n",
    "def _event_seq2snote_seq(event_sequence):\n",
    "    timeline = 0\n",
    "    velocity = 0\n",
    "    snote_seq = []\n",
    "\n",
    "    for event in event_sequence:\n",
    "        if event.type == 'time_shift':\n",
    "            timeline += ((event.value+1) / 100)\n",
    "        if event.type == 'velocity':\n",
    "            velocity = event.value * 4\n",
    "        else:\n",
    "            snote = SplitNote(event.type, timeline, event.value, velocity)\n",
    "            snote_seq.append(snote)\n",
    "    return snote_seq\n",
    "\n",
    "\n",
    "def _make_time_sift_events(prev_time, post_time):\n",
    "    time_interval = int(round((post_time - prev_time) * 100))\n",
    "    results = []\n",
    "    while time_interval >= RANGE_TIME_SHIFT:\n",
    "        results.append(Event(event_type='time_shift', value=RANGE_TIME_SHIFT-1))\n",
    "        time_interval -= RANGE_TIME_SHIFT\n",
    "    if time_interval == 0:\n",
    "        return results\n",
    "    else:\n",
    "        return results + [Event(event_type='time_shift', value=time_interval-1)]\n",
    "\n",
    "\n",
    "def _control_preprocess(ctrl_changes):\n",
    "    sustains = []\n",
    "\n",
    "    manager = None\n",
    "    for ctrl in ctrl_changes:\n",
    "        if ctrl.value >= 64 and manager is None:\n",
    "            # sustain down\n",
    "            manager = SustainDownManager(start=ctrl.time, end=None)\n",
    "        elif ctrl.value < 64 and manager is not None:\n",
    "            # sustain up\n",
    "            manager.end = ctrl.time\n",
    "            sustains.append(manager)\n",
    "            manager = None\n",
    "        elif ctrl.value < 64 and len(sustains) > 0:\n",
    "            sustains[-1].end = ctrl.time\n",
    "    return sustains\n",
    "\n",
    "\n",
    "def _note_preprocess(susteins, notes):\n",
    "    note_stream = []\n",
    "\n",
    "    if susteins:    # if the midi file has sustain controls\n",
    "        for sustain in susteins:\n",
    "            for note_idx, note in enumerate(notes):\n",
    "                if note.start < sustain.start:\n",
    "                    note_stream.append(note)\n",
    "                elif note.start > sustain.end:\n",
    "                    notes = notes[note_idx:]\n",
    "                    sustain.transposition_notes()\n",
    "                    break\n",
    "                else:\n",
    "                    sustain.add_managed_note(note)\n",
    "\n",
    "        for sustain in susteins:\n",
    "            note_stream += sustain.managed_notes\n",
    "    \n",
    "    else:       # else, just push everything into note stream\n",
    "        for note_idx, note in enumerate(notes):\n",
    "            note_stream.append(note)\n",
    "\n",
    "    note_stream.sort(key= lambda x: x.start)\n",
    "    return note_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EPianoDataset(Dataset):\n",
    "\n",
    "    def __init__(self, midi_list, max_seq=2048, random_seq=True):\n",
    "        self.max_seq    = max_seq\n",
    "        self.random_seq = random_seq\n",
    "        self.data_files = midi_list\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.data_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        raw_mid = torch.tensor(self.data_files[idx], dtype=TORCH_LABEL_TYPE, device=cpu_device())\n",
    "        x, tgt = process_midi(raw_mid, self.max_seq, self.random_seq)\n",
    "\n",
    "        return x, tgt\n",
    "    \n",
    "def process_midi(raw_mid, max_seq, random_seq):\n",
    "\n",
    "    x   = torch.full((max_seq, ), TOKEN_PAD, dtype=TORCH_LABEL_TYPE, device=cpu_device())\n",
    "    tgt = torch.full((max_seq, ), TOKEN_PAD, dtype=TORCH_LABEL_TYPE, device=cpu_device())\n",
    "\n",
    "    raw_len     = len(raw_mid)\n",
    "    full_seq    = max_seq + 1 # Performing seq2seq\n",
    "\n",
    "    if(raw_len == 0):\n",
    "        return x, tgt\n",
    "\n",
    "    if(raw_len < full_seq):\n",
    "        if tgt.shape[0] == raw_len:\n",
    "            #print(f'Tgt shape: {tgt.shape} Raw len: {raw_len} Skipping')\n",
    "            x[:raw_len]         = raw_mid\n",
    "            tgt[:raw_len-1]     = raw_mid[1:]\n",
    "            tgt[raw_len-1]        = TOKEN_END\n",
    "        else:\n",
    "            x[:raw_len]         = raw_mid\n",
    "            tgt[:raw_len-1]     = raw_mid[1:]\n",
    "            tgt[raw_len]        = TOKEN_END\n",
    "    else:\n",
    "        # Randomly selecting a range\n",
    "        if(random_seq):\n",
    "            end_range = raw_len - full_seq\n",
    "            start = random.randint(SEQUENCE_START, end_range)\n",
    "\n",
    "        # Always taking from the start to as far as we can\n",
    "        else:\n",
    "            start = SEQUENCE_START\n",
    "\n",
    "        end = start + full_seq\n",
    "\n",
    "        data = raw_mid[start:end]\n",
    "\n",
    "        x = data[:max_seq]\n",
    "        tgt = data[1:full_seq]\n",
    "\n",
    "    return x, tgt\n",
    "\n",
    "def decode_midi(idx_array, file_path=None):\n",
    "    event_sequence = [Event.from_int(idx) for idx in idx_array]\n",
    "    # print(event_sequence)\n",
    "    snote_seq = _event_seq2snote_seq(event_sequence)\n",
    "    note_seq = _merge_note(snote_seq)\n",
    "    note_seq.sort(key=lambda x:x.start)\n",
    "\n",
    "    mid = pretty_midi.PrettyMIDI()\n",
    "    # if want to change instument, see https://www.midi.org/specifications/item/gm-level-1-sound-set\n",
    "    instument = pretty_midi.Instrument(1, False, \"Generated by Music Transformer AI\")\n",
    "    instument.notes = note_seq\n",
    "\n",
    "    mid.instruments.append(instument)\n",
    "    if file_path is not None:\n",
    "        mid.write(file_path)\n",
    "    return mid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_workers = 1\n",
    "batch_size = 2\n",
    "random_seq = True\n",
    "\n",
    "rpr = False #'store_true'\n",
    "max_seq = 512 # Used later to generate primers\n",
    "n_layers = 6\n",
    "num_heads = 8\n",
    "d_model = 512\n",
    "dim_feedforward = 1024\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-saved train/val/test split loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "dict_keys(['train', 'val', 'test'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:44<00:00,  1.20it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('/home/storage/3020/db/K_cluster2_backup/TD/data/data_split.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "print(len(data))\n",
    "print(data.keys())\n",
    "\n",
    "\n",
    "test_list = []\n",
    "for file in tqdm(data['test']):\n",
    "    seqs = joblib.load(file)\n",
    "    for idx, seq in enumerate(seqs):\n",
    "        if len(seq) == 0:\n",
    "            continue\n",
    "        test_list.append(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test length: 34815\n"
     ]
    }
   ],
   "source": [
    "print(f'Test length: {len(test_list)}')\n",
    "test_dataset = EPianoDataset(test_list, max_seq, random_seq)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=n_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, n_layers=6, num_heads=8, d_model=512, dim_feedforward=1024,\n",
    "                 dropout=0.1, max_sequence=2048, rpr=False):\n",
    "        super(MusicTransformer, self).__init__()\n",
    "\n",
    "        self.dummy      = DummyDecoder()\n",
    "\n",
    "        self.nlayers    = n_layers\n",
    "        self.nhead      = num_heads\n",
    "        self.d_model    = d_model\n",
    "        self.d_ff       = dim_feedforward\n",
    "        self.dropout    = dropout\n",
    "        self.max_seq    = max_sequence\n",
    "        self.rpr        = rpr\n",
    "\n",
    "        self.embedding = nn.Embedding(VOCAB_SIZE, self.d_model)\n",
    "\n",
    "        self.positional_encoding = PositionalEncoding(self.d_model, self.dropout, self.max_seq)\n",
    "\n",
    "        if(not self.rpr):\n",
    "            self.transformer = nn.Transformer(\n",
    "                d_model=self.d_model, nhead=self.nhead, num_encoder_layers=self.nlayers,\n",
    "                num_decoder_layers=0, dropout=self.dropout, # activation=self.ff_activ,\n",
    "                dim_feedforward=self.d_ff, custom_decoder=self.dummy\n",
    "            )\n",
    "        # RPR Transformer\n",
    "        else:\n",
    "            encoder_norm = LayerNorm(self.d_model)\n",
    "            encoder_layer = TransformerEncoderLayerRPR(self.d_model, self.nhead, self.d_ff, self.dropout, er_len=self.max_seq)\n",
    "            encoder = TransformerEncoderRPR(encoder_layer, self.nlayers, encoder_norm)\n",
    "            self.transformer = nn.Transformer(\n",
    "                d_model=self.d_model, nhead=self.nhead, num_encoder_layers=self.nlayers,\n",
    "                num_decoder_layers=0, dropout=self.dropout,\n",
    "                dim_feedforward=self.d_ff, custom_decoder=self.dummy, custom_encoder=encoder\n",
    "            )\n",
    "\n",
    "        self.Wout       = nn.Linear(self.d_model, VOCAB_SIZE)\n",
    "        self.softmax    = nn.Softmax(dim=-1)\n",
    "\n",
    "    # forward\n",
    "    def forward(self, x, mask=True):\n",
    "        if(mask is True):\n",
    "            mask = self.transformer.generate_square_subsequent_mask(x.shape[1]).to(get_device())\n",
    "        else:\n",
    "            mask = None\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(1,0,2)\n",
    "        x = self.positional_encoding(x)\n",
    "        x_out = self.transformer(src=x, tgt=x, src_mask=mask)\n",
    "        x_out = x_out.permute(1,0,2)\n",
    "        y = self.Wout(x_out)\n",
    "        del mask\n",
    "        return y\n",
    "\n",
    "    # generate\n",
    "    def generate(self, primer=None, target_seq_length=1024, beam=0, beam_chance=1.0):\n",
    "\n",
    "        assert (not self.training), \"Cannot generate while in training mode\"\n",
    "\n",
    "        print(\"Generating sequence of max length:\", target_seq_length)\n",
    "\n",
    "        gen_seq = torch.full((1,target_seq_length), TOKEN_PAD, dtype=TORCH_LABEL_TYPE, device=get_device())\n",
    "\n",
    "        num_primer = len(primer)\n",
    "        gen_seq[..., :num_primer] = primer.type(TORCH_LABEL_TYPE).to(get_device())\n",
    "\n",
    "        cur_i = num_primer\n",
    "        while(cur_i < target_seq_length):\n",
    "\n",
    "            y = self.softmax(self.forward(gen_seq[..., :cur_i]))[..., :TOKEN_END]\n",
    "            token_probs = y[:, cur_i-1, :]\n",
    "\n",
    "            if(beam == 0):\n",
    "                beam_ran = 2.0\n",
    "            else:\n",
    "                beam_ran = random.uniform(0,1)\n",
    "\n",
    "            if(beam_ran <= beam_chance):\n",
    "                token_probs = token_probs.flatten()\n",
    "                top_res, top_i = torch.topk(token_probs, beam)\n",
    "\n",
    "                beam_rows = top_i // VOCAB_SIZE\n",
    "                beam_cols = top_i % VOCAB_SIZE\n",
    "\n",
    "                gen_seq = gen_seq[beam_rows, :]\n",
    "                gen_seq[..., cur_i] = beam_cols\n",
    "\n",
    "            else:\n",
    "                distrib = torch.distributions.categorical.Categorical(probs=token_probs)\n",
    "                next_token = distrib.sample()\n",
    "                gen_seq[:, cur_i] = next_token\n",
    "\n",
    "\n",
    "                # Let the transformer decide to end if it wants to\n",
    "                if(next_token == TOKEN_END):\n",
    "                    print(\"Model called end of sequence at:\", cur_i, \"/\", target_seq_length)\n",
    "                    break\n",
    "\n",
    "            cur_i += 1\n",
    "            if(cur_i % 50 == 0):\n",
    "                print(cur_i, \"/\", target_seq_length)\n",
    "\n",
    "        return gen_seq[:, :cur_i]\n",
    "\n",
    "class DummyDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DummyDecoder, self).__init__()\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask, memory_mask,tgt_key_padding_mask,memory_key_padding_mask):\n",
    "\n",
    "        return memory\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MusicTransformer(n_layers=n_layers, num_heads=num_heads,\n",
    "            d_model=d_model, dim_feedforward=dim_feedforward, dropout=dropout,\n",
    "            max_sequence=2048, rpr=rpr).to(get_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MusicTransformer(\n",
       "  (dummy): DummyDecoder()\n",
       "  (embedding): Embedding(390, 512)\n",
       "  (positional_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (3): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (4): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (5): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): DummyDecoder()\n",
       "  )\n",
       "  (Wout): Linear(in_features=512, out_features=390, bias=True)\n",
       "  (softmax): Softmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('/home/storage/3020/db/K_cluster2_backup/TD/gpt1_best_acc_bsize2.pth', map_location=get_device()))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random generation  \n",
    "Generate without using beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512])\n",
      "info removed pitch: 54\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pretty_midi.pretty_midi.PrettyMIDI at 0x7f9f6e5f9c10>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 0\n",
    "primer, _  = test_dataset[idx]\n",
    "primer = primer.to(get_device())\n",
    "print(primer.shape)\n",
    "\n",
    "out_dir = '/home/storage/3020/db/K_cluster2_backup/TD/gpt1_examples'\n",
    "save_path = os.path.join(out_dir, f\"primer_{idx}.mid\")\n",
    "decode_midi(primer[:max_seq].cpu().numpy(), file_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating sequence of max length: 2048\n",
      "550 / 2048\n",
      "600 / 2048\n",
      "650 / 2048\n",
      "700 / 2048\n",
      "750 / 2048\n",
      "800 / 2048\n",
      "850 / 2048\n",
      "900 / 2048\n",
      "950 / 2048\n",
      "1000 / 2048\n",
      "1050 / 2048\n",
      "1100 / 2048\n",
      "1150 / 2048\n",
      "1200 / 2048\n",
      "1250 / 2048\n",
      "1300 / 2048\n",
      "1350 / 2048\n",
      "1400 / 2048\n",
      "1450 / 2048\n",
      "1500 / 2048\n",
      "1550 / 2048\n",
      "1600 / 2048\n",
      "1650 / 2048\n",
      "1700 / 2048\n",
      "1750 / 2048\n",
      "1800 / 2048\n",
      "1850 / 2048\n",
      "1900 / 2048\n",
      "1950 / 2048\n",
      "2000 / 2048\n",
      "info removed pitch: 66\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pretty_midi.pretty_midi.PrettyMIDI at 0x7f9f6e63dad0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_seq_length = 2048\n",
    "rand_seq = model.generate(primer[:max_seq], target_seq_length, beam=0)\n",
    "\n",
    "f_path = os.path.join(out_dir, f\"rand_{idx}.mid\")\n",
    "decode_midi(rand_seq[0].cpu().numpy(), file_path=f_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beam search generation  \n",
    "Generate from same primer using beam search of 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info removed pitch: 54\n",
      "info removed pitch: 66\n",
      "info removed pitch: 62\n",
      "info removed pitch: 57\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pretty_midi.pretty_midi.PrettyMIDI at 0x7f0e4fd0a990>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 0\n",
    "primer, _  = test_dataset[idx]\n",
    "primer = primer.to(get_device())\n",
    "out_dir = '/home/storage/3020/db/K_cluster2_backup/TD/gpt1_examples'\n",
    "save_path = os.path.join(out_dir, f\"primerbeam_{idx}.mid\")\n",
    "decode_midi(primer[:max_seq].cpu().numpy(), file_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating sequence of max length: 2048\n",
      "550 / 2048\n",
      "600 / 2048\n",
      "650 / 2048\n",
      "700 / 2048\n",
      "750 / 2048\n",
      "800 / 2048\n",
      "850 / 2048\n",
      "900 / 2048\n",
      "950 / 2048\n",
      "1000 / 2048\n",
      "1050 / 2048\n",
      "1100 / 2048\n",
      "1150 / 2048\n",
      "1200 / 2048\n",
      "1250 / 2048\n",
      "1300 / 2048\n",
      "1350 / 2048\n",
      "1400 / 2048\n",
      "1450 / 2048\n",
      "1500 / 2048\n",
      "1550 / 2048\n",
      "1600 / 2048\n",
      "1650 / 2048\n",
      "1700 / 2048\n",
      "1750 / 2048\n",
      "1800 / 2048\n",
      "1850 / 2048\n",
      "1900 / 2048\n",
      "1950 / 2048\n",
      "2000 / 2048\n",
      "info removed pitch: 54\n",
      "info removed pitch: 66\n",
      "info removed pitch: 62\n",
      "info removed pitch: 57\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 29\n",
      "info removed pitch: 126\n",
      "info removed pitch: 41\n",
      "info removed pitch: 27\n",
      "info removed pitch: 24\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 55\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 59\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 22\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 104\n",
      "info removed pitch: 126\n",
      "info removed pitch: 20\n",
      "info removed pitch: 126\n",
      "info removed pitch: 22\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 53\n",
      "info removed pitch: 59\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 104\n",
      "info removed pitch: 104\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 70\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 59\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 53\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 73\n",
      "info removed pitch: 126\n",
      "info removed pitch: 73\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 64\n",
      "info removed pitch: 126\n",
      "info removed pitch: 64\n",
      "info removed pitch: 126\n",
      "info removed pitch: 55\n",
      "info removed pitch: 126\n",
      "info removed pitch: 64\n",
      "info removed pitch: 126\n",
      "info removed pitch: 55\n",
      "info removed pitch: 126\n",
      "info removed pitch: 55\n",
      "info removed pitch: 126\n",
      "info removed pitch: 55\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 59\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 70\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 51\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 73\n",
      "info removed pitch: 73\n",
      "info removed pitch: 73\n",
      "info removed pitch: 126\n",
      "info removed pitch: 84\n",
      "info removed pitch: 73\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 73\n",
      "info removed pitch: 126\n",
      "info removed pitch: 53\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 70\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 59\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 55\n",
      "info removed pitch: 126\n",
      "info removed pitch: 55\n",
      "info removed pitch: 55\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 82\n",
      "info removed pitch: 126\n",
      "info removed pitch: 53\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 59\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 73\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 70\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 89\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 53\n",
      "info removed pitch: 126\n",
      "info removed pitch: 89\n",
      "info removed pitch: 126\n",
      "info removed pitch: 70\n",
      "info removed pitch: 126\n",
      "info removed pitch: 72\n",
      "info removed pitch: 56\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 89\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 58\n",
      "info removed pitch: 127\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 53\n",
      "info removed pitch: 126\n",
      "info removed pitch: 53\n",
      "info removed pitch: 53\n",
      "info removed pitch: 53\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 59\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 59\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 70\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 53\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 89\n",
      "info removed pitch: 126\n",
      "info removed pitch: 70\n",
      "info removed pitch: 72\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 59\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 53\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 51\n",
      "info removed pitch: 51\n",
      "info removed pitch: 126\n",
      "info removed pitch: 51\n",
      "info removed pitch: 126\n",
      "info removed pitch: 51\n",
      "info removed pitch: 126\n",
      "info removed pitch: 51\n",
      "info removed pitch: 126\n",
      "info removed pitch: 51\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 59\n",
      "info removed pitch: 126\n",
      "info removed pitch: 70\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 70\n",
      "info removed pitch: 70\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 65\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 65\n",
      "info removed pitch: 126\n",
      "info removed pitch: 65\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 70\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 59\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 59\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 70\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 59\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 70\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 51\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 51\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 51\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 59\n",
      "info removed pitch: 126\n",
      "info removed pitch: 70\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 51\n",
      "info removed pitch: 126\n",
      "info removed pitch: 51\n",
      "info removed pitch: 126\n",
      "info removed pitch: 51\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 59\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 70\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 70\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 59\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 59\n",
      "info removed pitch: 84\n",
      "info removed pitch: 84\n",
      "info removed pitch: 89\n",
      "info removed pitch: 89\n",
      "info removed pitch: 84\n",
      "info removed pitch: 84\n",
      "info removed pitch: 84\n",
      "info removed pitch: 89\n",
      "info removed pitch: 70\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 59\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 59\n",
      "info removed pitch: 126\n",
      "info removed pitch: 70\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 84\n",
      "info removed pitch: 59\n",
      "info removed pitch: 82\n",
      "info removed pitch: 82\n",
      "info removed pitch: 84\n",
      "info removed pitch: 126\n",
      "info removed pitch: 70\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 70\n",
      "info removed pitch: 126\n",
      "info removed pitch: 70\n",
      "info removed pitch: 126\n",
      "info removed pitch: 70\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 70\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 59\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pretty_midi.pretty_midi.PrettyMIDI at 0x7f0e7e7f8910>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_seq_length = 2048\n",
    "beam = 2\n",
    "beam_seq = model.generate(primer[:max_seq], target_seq_length, beam=beam)\n",
    "\n",
    "f_path = os.path.join(out_dir, f\"beam{beam}_{idx}.mid\")\n",
    "\n",
    "decode_midi(beam_seq[0].cpu().numpy(), file_path=f_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
