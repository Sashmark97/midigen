{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pretty_midi\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.nn.modules.normalization import LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:32<00:00,  1.36it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_pickle = '/home/storage/3020/db/K_cluster2_backup/TD/data/data_split.pkl'\n",
    "with open(dataset_pickle, 'rb') as f:\n",
    "    files = pickle.load(f)\n",
    "\n",
    "test_list = []\n",
    "    \n",
    "for file in tqdm(files['test']):\n",
    "    seqs = joblib.load(file)\n",
    "    for idx, seq in enumerate(seqs):\n",
    "        if len(seq) == 0:\n",
    "            continue\n",
    "        test_list.append(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_START = 0\n",
    "RANGE_NOTE_ON = 128\n",
    "RANGE_NOTE_OFF = 128\n",
    "RANGE_VEL = 32\n",
    "RANGE_TIME_SHIFT = 100\n",
    "\n",
    "START_IDX = {\n",
    "    'note_on': 0,\n",
    "    'note_off': RANGE_NOTE_ON,\n",
    "    'time_shift': RANGE_NOTE_ON + RANGE_NOTE_OFF,\n",
    "    'velocity': RANGE_NOTE_ON + RANGE_NOTE_OFF + RANGE_TIME_SHIFT\n",
    "}\n",
    "\n",
    "\n",
    "SEPERATOR               = \"=========================\"\n",
    "\n",
    "# Taken from the paper\n",
    "ADAM_BETA_1             = 0.9\n",
    "ADAM_BETA_2             = 0.98\n",
    "ADAM_EPSILON            = 10e-9\n",
    "\n",
    "LR_DEFAULT_START        = 1.0\n",
    "SCHEDULER_WARMUP_STEPS  = 4000\n",
    "# LABEL_SMOOTHING_E       = 0.1\n",
    "\n",
    "# DROPOUT_P               = 0.1\n",
    "\n",
    "TOKEN_END               = RANGE_NOTE_ON + RANGE_NOTE_OFF + RANGE_VEL + RANGE_TIME_SHIFT\n",
    "TOKEN_PAD               = TOKEN_END + 1\n",
    "\n",
    "VOCAB_SIZE              = TOKEN_PAD + 1\n",
    "\n",
    "TORCH_FLOAT             = torch.float32\n",
    "TORCH_INT               = torch.int32\n",
    "\n",
    "TORCH_LABEL_TYPE        = torch.long\n",
    "\n",
    "PREPEND_ZEROS_WIDTH     = 4\n",
    "\n",
    "TORCH_CPU_DEVICE = torch.device(\"cpu\")\n",
    "USE_CUDA = 1\n",
    "TORCH_CUDA_DEVICE = torch.device(\"cuda:9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cpu_device():\n",
    "\n",
    "    return TORCH_CPU_DEVICE\n",
    "\n",
    "def get_device():\n",
    "    if((not USE_CUDA) or (TORCH_CUDA_DEVICE is None)):\n",
    "        return TORCH_CPU_DEVICE\n",
    "    else:\n",
    "        return TORCH_CUDA_DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-2 generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SustainAdapter:\n",
    "    def __init__(self, time, type):\n",
    "        self.start =  time\n",
    "        self.type = type\n",
    "\n",
    "\n",
    "class SustainDownManager:\n",
    "    def __init__(self, start, end):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.managed_notes = []\n",
    "        self._note_dict = {} # key: pitch, value: note.start\n",
    "\n",
    "    def add_managed_note(self, note: pretty_midi.Note):\n",
    "        self.managed_notes.append(note)\n",
    "\n",
    "    def transposition_notes(self):\n",
    "        for note in reversed(self.managed_notes):\n",
    "            try:\n",
    "                note.end = self._note_dict[note.pitch]\n",
    "            except KeyError:\n",
    "                note.end = max(self.end, note.end)\n",
    "            self._note_dict[note.pitch] = note.start\n",
    "\n",
    "\n",
    "# Divided note by note_on, note_off\n",
    "class SplitNote:\n",
    "    def __init__(self, type, time, value, velocity):\n",
    "        ## type: note_on, note_off\n",
    "        self.type = type\n",
    "        self.time = time\n",
    "        self.velocity = velocity\n",
    "        self.value = value\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '<[SNote] time: {} type: {}, value: {}, velocity: {}>'\\\n",
    "            .format(self.time, self.type, self.value, self.velocity)\n",
    "\n",
    "\n",
    "class Event:\n",
    "    def __init__(self, event_type, value):\n",
    "        self.type = event_type\n",
    "        self.value = value\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '<Event type: {}, value: {}>'.format(self.type, self.value)\n",
    "\n",
    "    def to_int(self):\n",
    "        return START_IDX[self.type] + self.value\n",
    "\n",
    "    @staticmethod\n",
    "    def from_int(int_value):\n",
    "        info = Event._type_check(int_value)\n",
    "        return Event(info['type'], info['value'])\n",
    "\n",
    "    @staticmethod\n",
    "    def _type_check(int_value):\n",
    "        range_note_on = range(0, RANGE_NOTE_ON)\n",
    "        range_note_off = range(RANGE_NOTE_ON, RANGE_NOTE_ON+RANGE_NOTE_OFF)\n",
    "        range_time_shift = range(RANGE_NOTE_ON+RANGE_NOTE_OFF,RANGE_NOTE_ON+RANGE_NOTE_OFF+RANGE_TIME_SHIFT)\n",
    "\n",
    "        valid_value = int_value\n",
    "\n",
    "        if int_value in range_note_on:\n",
    "            return {'type': 'note_on', 'value': valid_value}\n",
    "        elif int_value in range_note_off:\n",
    "            valid_value -= RANGE_NOTE_ON\n",
    "            return {'type': 'note_off', 'value': valid_value}\n",
    "        elif int_value in range_time_shift:\n",
    "            valid_value -= (RANGE_NOTE_ON + RANGE_NOTE_OFF)\n",
    "            return {'type': 'time_shift', 'value': valid_value}\n",
    "        else:\n",
    "            valid_value -= (RANGE_NOTE_ON + RANGE_NOTE_OFF + RANGE_TIME_SHIFT)\n",
    "            return {'type': 'velocity', 'value': valid_value}\n",
    "\n",
    "\n",
    "def _divide_note(notes):\n",
    "    result_array = []\n",
    "    notes.sort(key=lambda x: x.start)\n",
    "\n",
    "    for note in notes:\n",
    "        on = SplitNote('note_on', note.start, note.pitch, note.velocity)\n",
    "        off = SplitNote('note_off', note.end, note.pitch, None)\n",
    "        result_array += [on, off]\n",
    "    return result_array\n",
    "\n",
    "\n",
    "def _merge_note(snote_sequence):\n",
    "    note_on_dict = {}\n",
    "    result_array = []\n",
    "\n",
    "    for snote in snote_sequence:\n",
    "        # print(note_on_dict)\n",
    "        if snote.type == 'note_on':\n",
    "            note_on_dict[snote.value] = snote\n",
    "        elif snote.type == 'note_off':\n",
    "            try:\n",
    "                on = note_on_dict[snote.value]\n",
    "                off = snote\n",
    "                if off.time - on.time == 0:\n",
    "                    continue\n",
    "                result = pretty_midi.Note(on.velocity, snote.value, on.time, off.time)\n",
    "                result_array.append(result)\n",
    "            except:\n",
    "                print('info removed pitch: {}'.format(snote.value))\n",
    "    return result_array\n",
    "\n",
    "\n",
    "def _snote2events(snote: SplitNote, prev_vel: int):\n",
    "    result = []\n",
    "    if snote.velocity is not None:\n",
    "        modified_velocity = snote.velocity // 4\n",
    "        if prev_vel != modified_velocity:\n",
    "            result.append(Event(event_type='velocity', value=modified_velocity))\n",
    "    result.append(Event(event_type=snote.type, value=snote.value))\n",
    "    return result\n",
    "\n",
    "\n",
    "def _event_seq2snote_seq(event_sequence):\n",
    "    timeline = 0\n",
    "    velocity = 0\n",
    "    snote_seq = []\n",
    "\n",
    "    for event in event_sequence:\n",
    "        if event.type == 'time_shift':\n",
    "            timeline += ((event.value+1) / 100)\n",
    "        if event.type == 'velocity':\n",
    "            velocity = event.value * 4\n",
    "        else:\n",
    "            snote = SplitNote(event.type, timeline, event.value, velocity)\n",
    "            snote_seq.append(snote)\n",
    "    return snote_seq\n",
    "\n",
    "\n",
    "def _make_time_sift_events(prev_time, post_time):\n",
    "    time_interval = int(round((post_time - prev_time) * 100))\n",
    "    results = []\n",
    "    while time_interval >= RANGE_TIME_SHIFT:\n",
    "        results.append(Event(event_type='time_shift', value=RANGE_TIME_SHIFT-1))\n",
    "        time_interval -= RANGE_TIME_SHIFT\n",
    "    if time_interval == 0:\n",
    "        return results\n",
    "    else:\n",
    "        return results + [Event(event_type='time_shift', value=time_interval-1)]\n",
    "\n",
    "\n",
    "def _control_preprocess(ctrl_changes):\n",
    "    sustains = []\n",
    "\n",
    "    manager = None\n",
    "    for ctrl in ctrl_changes:\n",
    "        if ctrl.value >= 64 and manager is None:\n",
    "            # sustain down\n",
    "            manager = SustainDownManager(start=ctrl.time, end=None)\n",
    "        elif ctrl.value < 64 and manager is not None:\n",
    "            # sustain up\n",
    "            manager.end = ctrl.time\n",
    "            sustains.append(manager)\n",
    "            manager = None\n",
    "        elif ctrl.value < 64 and len(sustains) > 0:\n",
    "            sustains[-1].end = ctrl.time\n",
    "    return sustains\n",
    "\n",
    "\n",
    "def _note_preprocess(susteins, notes):\n",
    "    note_stream = []\n",
    "\n",
    "    if susteins:    # if the midi file has sustain controls\n",
    "        for sustain in susteins:\n",
    "            for note_idx, note in enumerate(notes):\n",
    "                if note.start < sustain.start:\n",
    "                    note_stream.append(note)\n",
    "                elif note.start > sustain.end:\n",
    "                    notes = notes[note_idx:]\n",
    "                    sustain.transposition_notes()\n",
    "                    break\n",
    "                else:\n",
    "                    sustain.add_managed_note(note)\n",
    "\n",
    "        for sustain in susteins:\n",
    "            note_stream += sustain.managed_notes\n",
    "    \n",
    "    else:       # else, just push everything into note stream\n",
    "        for note_idx, note in enumerate(notes):\n",
    "            note_stream.append(note)\n",
    "\n",
    "    note_stream.sort(key= lambda x: x.start)\n",
    "    return note_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EPianoDataset(Dataset):\n",
    "\n",
    "    def __init__(self, midi_list, max_seq=2048, random_seq=True):\n",
    "        self.max_seq    = max_seq\n",
    "        self.random_seq = random_seq\n",
    "        self.data_files = midi_list\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.data_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        raw_mid = torch.tensor(self.data_files[idx], dtype=TORCH_LABEL_TYPE, device=cpu_device())\n",
    "        x, tgt = process_midi(raw_mid, self.max_seq, self.random_seq)\n",
    "\n",
    "        return x, tgt\n",
    "    \n",
    "def process_midi(raw_mid, max_seq, random_seq):\n",
    "\n",
    "    x   = torch.full((max_seq, ), TOKEN_PAD, dtype=TORCH_LABEL_TYPE, device=cpu_device())\n",
    "    tgt = torch.full((max_seq, ), TOKEN_PAD, dtype=TORCH_LABEL_TYPE, device=cpu_device())\n",
    "\n",
    "    raw_len     = len(raw_mid)\n",
    "    full_seq    = max_seq + 1 # Performing seq2seq\n",
    "\n",
    "    if(raw_len == 0):\n",
    "        return x, tgt\n",
    "\n",
    "    if(raw_len < full_seq):\n",
    "        if tgt.shape[0] == raw_len:\n",
    "            #print(f'Tgt shape: {tgt.shape} Raw len: {raw_len} Skipping')\n",
    "            x[:raw_len]         = raw_mid\n",
    "            tgt[:raw_len-1]     = raw_mid[1:]\n",
    "            tgt[raw_len-1]        = TOKEN_END\n",
    "        else:\n",
    "            x[:raw_len]         = raw_mid\n",
    "            tgt[:raw_len-1]     = raw_mid[1:]\n",
    "            tgt[raw_len]        = TOKEN_END\n",
    "    else:\n",
    "        # Randomly selecting a range\n",
    "        if(random_seq):\n",
    "            end_range = raw_len - full_seq\n",
    "            start = random.randint(SEQUENCE_START, end_range)\n",
    "\n",
    "        # Always taking from the start to as far as we can\n",
    "        else:\n",
    "            start = SEQUENCE_START\n",
    "\n",
    "        end = start + full_seq\n",
    "\n",
    "        data = raw_mid[start:end]\n",
    "\n",
    "        x = data[:max_seq]\n",
    "        tgt = data[1:full_seq]\n",
    "\n",
    "    return x, tgt\n",
    "\n",
    "def decode_midi(idx_array, file_path=None):\n",
    "    event_sequence = [Event.from_int(idx) for idx in idx_array]\n",
    "    # print(event_sequence)\n",
    "    snote_seq = _event_seq2snote_seq(event_sequence)\n",
    "    note_seq = _merge_note(snote_seq)\n",
    "    note_seq.sort(key=lambda x:x.start)\n",
    "\n",
    "    mid = pretty_midi.PrettyMIDI()\n",
    "    # if want to change instument, see https://www.midi.org/specifications/item/gm-level-1-sound-set\n",
    "    instument = pretty_midi.Instrument(1, False, \"Generated by Music Transformer AI\")\n",
    "    instument.notes = note_seq\n",
    "\n",
    "    mid.instruments.append(instument)\n",
    "    if file_path is not None:\n",
    "        mid.write(file_path)\n",
    "    return mid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_workers = 1\n",
    "batch_size = 2\n",
    "random_seq = True\n",
    "\n",
    "max_seq = 512 # Used later to generate primers\n",
    "n_layers = 6\n",
    "num_heads = 8\n",
    "d_model = 512\n",
    "dim_feedforward = 1024\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-saved train/val/test split loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test length: 34815\n"
     ]
    }
   ],
   "source": [
    "print(f'Test length: {len(test_list)}')\n",
    "test_dataset = EPianoDataset(test_list, max_seq, random_seq)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=n_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A vanilla multi-head masked self-attention layer with a projection at the end.\n",
    "    It is possible to use torch.nn.MultiheadAttention here but I am including an\n",
    "    explicit implementation here to show that there is nothing too scary here.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads\n",
    "        self.key = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.query = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.value = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # regularization\n",
    "        self.attn_drop = nn.Dropout(config.attn_pdrop)\n",
    "        self.resid_drop = nn.Dropout(config.resid_pdrop)\n",
    "        # output projection\n",
    "        self.proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                     .view(1, 1, config.block_size, config.block_size))\n",
    "        self.n_head = config.n_head\n",
    "\n",
    "    def forward(self, x, layer_past=None):\n",
    "        B, T, C = x.size()\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        k = self.key(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = self.query(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = self.value(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.mask[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_drop(att)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_drop(self.proj(y))\n",
    "        return y\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" an unassuming Transformer block \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
    "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "            nn.Dropout(config.resid_pdrop),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    \"\"\"  the full GPT language model, with a context size of block_size \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        # input embedding stem\n",
    "        self.tok_emb = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, config.block_size, config.n_embd))\n",
    "        self.drop = nn.Dropout(config.embd_pdrop)\n",
    "        # transformer\n",
    "        self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layer)])\n",
    "        # decoder head\n",
    "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
    "        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        self.block_size = config.block_size\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "        logger.info(\"number of parameters: %e\", sum(p.numel() for p in self.parameters()))\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.block_size\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def configure_optimizers(self, train_config):\n",
    "        \"\"\"\n",
    "        This long function is unfortunately doing something very simple and is being very defensive:\n",
    "        We are separating out all parameters of the model into two buckets: those that will experience\n",
    "        weight decay for regularization and those that won't (biases, and layernorm/embedding weights).\n",
    "        We are then returning the PyTorch optimizer object.\n",
    "        \"\"\"\n",
    "\n",
    "        # separate out all parameters to those that will and won't experience regularizing weight decay\n",
    "        decay = set()\n",
    "        no_decay = set()\n",
    "        whitelist_weight_modules = (torch.nn.Linear, )\n",
    "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
    "        for mn, m in self.named_modules():\n",
    "            for pn, p in m.named_parameters():\n",
    "                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n",
    "\n",
    "                if pn.endswith('bias'):\n",
    "                    # all biases will not be decayed\n",
    "                    no_decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
    "                    # weights of whitelist modules will be weight decayed\n",
    "                    decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
    "                    # weights of blacklist modules will NOT be weight decayed\n",
    "                    no_decay.add(fpn)\n",
    "\n",
    "        # special case the position embedding parameter in the root GPT module as not decayed\n",
    "        no_decay.add('pos_emb')\n",
    "\n",
    "        # validate that we considered every parameter\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        inter_params = decay & no_decay\n",
    "        union_params = decay | no_decay\n",
    "        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
    "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
    "                                                    % (str(param_dict.keys() - union_params), )\n",
    "\n",
    "        # create the pytorch optimizer object\n",
    "        optim_groups = [\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.weight_decay},\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=train_config.learning_rate, betas=train_config.betas)\n",
    "        return optimizer\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.block_size, \"Cannot forward, model block size is exhausted.\"\n",
    "\n",
    "        # forward the GPT model\n",
    "        token_embeddings = self.tok_emb(idx) # each index maps to a (learnable) vector\n",
    "        position_embeddings = self.pos_emb[:, :t, :] # each position maps to a (learnable) vector\n",
    "        x = self.drop(token_embeddings + position_embeddings)\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "\n",
    "        # if we are given some desired targets also calculate the loss\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    # generate\n",
    "    def generate(self, primer=None, target_seq_length=1024, beam=0, beam_chance=1.0):\n",
    "\n",
    "        assert (not self.training), \"Cannot generate while in training mode\"\n",
    "\n",
    "        print(\"Generating sequence of max length:\", target_seq_length)\n",
    "\n",
    "        gen_seq = torch.full((1,target_seq_length), TOKEN_PAD, dtype=TORCH_LABEL_TYPE, device=get_device())\n",
    "\n",
    "        num_primer = len(primer)\n",
    "        gen_seq[..., :num_primer] = primer.type(TORCH_LABEL_TYPE).to(get_device())\n",
    "\n",
    "        cur_i = num_primer\n",
    "        while(cur_i < target_seq_length):\n",
    "            logits, _ = self.forward(gen_seq[..., :cur_i])\n",
    "            y = self.softmax(logits)[..., :TOKEN_END]\n",
    "            token_probs = y[:, cur_i-1, :]\n",
    "\n",
    "            if(beam == 0):\n",
    "                beam_ran = 2.0\n",
    "            else:\n",
    "                beam_ran = random.uniform(0,1)\n",
    "\n",
    "            if(beam_ran <= beam_chance):\n",
    "                token_probs = token_probs.flatten()\n",
    "                top_res, top_i = torch.topk(token_probs, beam)\n",
    "\n",
    "                beam_rows = top_i // VOCAB_SIZE\n",
    "                beam_cols = top_i % VOCAB_SIZE\n",
    "\n",
    "                gen_seq = gen_seq[beam_rows, :]\n",
    "                gen_seq[..., cur_i] = beam_cols\n",
    "\n",
    "            else:\n",
    "                distrib = torch.distributions.categorical.Categorical(probs=token_probs)\n",
    "                next_token = distrib.sample()\n",
    "                gen_seq[:, cur_i] = next_token\n",
    "\n",
    "\n",
    "                # Let the transformer decide to end if it wants to\n",
    "                if(next_token == TOKEN_END):\n",
    "                    print(\"Model called end of sequence at:\", cur_i, \"/\", target_seq_length)\n",
    "                    break\n",
    "\n",
    "            cur_i += 1\n",
    "            if(cur_i % 50 == 0):\n",
    "                print(cur_i, \"/\", target_seq_length)\n",
    "\n",
    "        return gen_seq[:, :cur_i]\n",
    "\n",
    "class DummyDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DummyDecoder, self).__init__()\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask, memory_mask,tgt_key_padding_mask,memory_key_padding_mask):\n",
    "\n",
    "        return memory\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "class GPTConfig:\n",
    "    \"\"\" base GPT config, params common to all GPT versions \"\"\"\n",
    "    embd_pdrop = 0.1\n",
    "    resid_pdrop = 0.1\n",
    "    attn_pdrop = 0.1\n",
    "\n",
    "    def __init__(self, vocab_size, block_size, **kwargs):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        for k,v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (tok_emb): Embedding(390, 512)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): CausalSelfAttention(\n",
       "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Block(\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): CausalSelfAttention(\n",
       "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): Block(\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): CausalSelfAttention(\n",
       "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): Block(\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): CausalSelfAttention(\n",
       "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): Block(\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): CausalSelfAttention(\n",
       "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): Block(\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): CausalSelfAttention(\n",
       "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (head): Linear(in_features=512, out_features=390, bias=False)\n",
       "  (softmax): Softmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = GPTConfig(VOCAB_SIZE, \n",
    "                  2048,\n",
    "                  n_layer=6, \n",
    "                  n_head=8, \n",
    "                  n_embd=512)\n",
    "model = GPT(config).to(get_device())\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (tok_emb): Embedding(390, 512)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): CausalSelfAttention(\n",
       "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Block(\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): CausalSelfAttention(\n",
       "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): Block(\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): CausalSelfAttention(\n",
       "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): Block(\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): CausalSelfAttention(\n",
       "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): Block(\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): CausalSelfAttention(\n",
       "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): Block(\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): CausalSelfAttention(\n",
       "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (head): Linear(in_features=512, out_features=390, bias=False)\n",
       "  (softmax): Softmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('/home/storage/3020/db/K_cluster2_backup/TD/gpt2_exp/gpt2_best_acc_bsize2.pth', map_location=get_device()))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random generation  \n",
    "Generate without using beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512])\n",
      "info removed pitch: 54\n",
      "info removed pitch: 66\n",
      "info removed pitch: 62\n",
      "info removed pitch: 57\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pretty_midi.pretty_midi.PrettyMIDI at 0x7f6e64cedfd0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 0\n",
    "primer, _  = test_dataset[idx]\n",
    "primer = primer.to(get_device())\n",
    "print(primer.shape)\n",
    "\n",
    "out_dir = '/home/storage/3020/db/K_cluster2_backup/TD/gpt2_examples'\n",
    "save_path = os.path.join(out_dir, f\"primer_{idx}.mid\")\n",
    "decode_midi(primer[:max_seq].cpu().numpy(), file_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating sequence of max length: 2048\n",
      "550 / 2048\n",
      "600 / 2048\n",
      "650 / 2048\n",
      "700 / 2048\n",
      "750 / 2048\n",
      "800 / 2048\n",
      "850 / 2048\n",
      "900 / 2048\n",
      "950 / 2048\n",
      "1000 / 2048\n",
      "1050 / 2048\n",
      "1100 / 2048\n",
      "1150 / 2048\n",
      "1200 / 2048\n",
      "1250 / 2048\n",
      "1300 / 2048\n",
      "1350 / 2048\n",
      "1400 / 2048\n",
      "1450 / 2048\n",
      "1500 / 2048\n",
      "1550 / 2048\n",
      "1600 / 2048\n",
      "1650 / 2048\n",
      "1700 / 2048\n",
      "1750 / 2048\n",
      "1800 / 2048\n",
      "1850 / 2048\n",
      "1900 / 2048\n",
      "1950 / 2048\n",
      "2000 / 2048\n",
      "info removed pitch: 54\n",
      "info removed pitch: 66\n",
      "info removed pitch: 62\n",
      "info removed pitch: 57\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pretty_midi.pretty_midi.PrettyMIDI at 0x7f6e64d42210>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_seq_length = 2048\n",
    "rand_seq = model.generate(primer[:max_seq], target_seq_length, beam=0)\n",
    "\n",
    "f_path = os.path.join(out_dir, f\"rand_{idx}.mid\")\n",
    "decode_midi(rand_seq[0].cpu().numpy(), file_path=f_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beam search generation  \n",
    "Generate from same primer using beam search of 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pretty_midi.pretty_midi.PrettyMIDI at 0x7f6e64df3b50>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 0\n",
    "primer, _  = test_dataset[idx]\n",
    "primer = primer.to(get_device())\n",
    "out_dir = '/home/storage/3020/db/K_cluster2_backup/TD/gpt2_examples'\n",
    "save_path = os.path.join(out_dir, f\"primerbeam_{idx}.mid\")\n",
    "decode_midi(primer[:max_seq].cpu().numpy(), file_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating sequence of max length: 2048\n",
      "550 / 2048\n",
      "600 / 2048\n",
      "650 / 2048\n",
      "700 / 2048\n",
      "750 / 2048\n",
      "800 / 2048\n",
      "850 / 2048\n",
      "900 / 2048\n",
      "950 / 2048\n",
      "1000 / 2048\n",
      "1050 / 2048\n",
      "1100 / 2048\n",
      "1150 / 2048\n",
      "1200 / 2048\n",
      "1250 / 2048\n",
      "1300 / 2048\n",
      "1350 / 2048\n",
      "1400 / 2048\n",
      "1450 / 2048\n",
      "1500 / 2048\n",
      "1550 / 2048\n",
      "1600 / 2048\n",
      "1650 / 2048\n",
      "1700 / 2048\n",
      "1750 / 2048\n",
      "1800 / 2048\n",
      "1850 / 2048\n",
      "1900 / 2048\n",
      "1950 / 2048\n",
      "2000 / 2048\n",
      "info removed pitch: 62\n",
      "info removed pitch: 126\n",
      "info removed pitch: 58\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 27\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 39\n",
      "info removed pitch: 72\n",
      "info removed pitch: 72\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 70\n",
      "info removed pitch: 126\n",
      "info removed pitch: 59\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 70\n",
      "info removed pitch: 126\n",
      "info removed pitch: 73\n",
      "info removed pitch: 70\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 39\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 39\n",
      "info removed pitch: 126\n",
      "info removed pitch: 39\n",
      "info removed pitch: 126\n",
      "info removed pitch: 56\n",
      "info removed pitch: 56\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 82\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 39\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 39\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 39\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 51\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 56\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 39\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 56\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 39\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 56\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 39\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 39\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 39\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 39\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 39\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 39\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 39\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 39\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 39\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 39\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 39\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 39\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 39\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 39\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 39\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 39\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 39\n",
      "info removed pitch: 126\n",
      "info removed pitch: 39\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 39\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 39\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 126\n",
      "info removed pitch: 127\n",
      "info removed pitch: 126\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pretty_midi.pretty_midi.PrettyMIDI at 0x7f6e64e255d0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_seq_length = 2048\n",
    "beam = 2\n",
    "beam_seq = model.generate(primer[:max_seq], target_seq_length, beam=beam)\n",
    "\n",
    "f_path = os.path.join(out_dir, f\"beam{beam}_{idx}.mid\")\n",
    "\n",
    "decode_midi(beam_seq[0].cpu().numpy(), file_path=f_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
